{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/NN-project-memorizing-transformers/blob/main/NN_project_Antonelli_DeSantis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authors\n",
        "\n",
        "* **Luigi Antonelli**\n",
        "\n",
        "* **Lorenzo De Santis**"
      ],
      "metadata": {
        "id": "cwK81vCpEiRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH_q6bacGaqq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from pathlib import Path\n",
        "from filelock import FileLock\n",
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a4BTaVB5Pxs",
        "outputId": "a71b7759-1864-4abc-90a5-92030fe74b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Memory"
      ],
      "metadata": {
        "id": "4I2ce2jPLzle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QppURbZ0KL0f",
        "outputId": "bbe5865c-fc82-4419-9f42-d1e3e3c3b1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.8/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krC3-klsLPVa",
        "outputId": "4cd1f5f6-b1f7-41b6-9335-28a5d658d187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import per la knn memory\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from functools import wraps\n",
        "\n",
        "from contextlib import ExitStack, contextmanager\n",
        "\n",
        "from einops import rearrange, pack, unpack\n",
        "\n",
        "# multiprocessing\n",
        "\n",
        "from joblib import Parallel, delayed, cpu_count"
      ],
      "metadata": {
        "id": "KeHkdSn0KBSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FAISS_INDEX_GPU_ID = int(os.getenv('FAISS_INDEX_GPU_ID', 0))\n",
        "\n",
        "DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY = './.tmp/knn.memories'\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_list(val):\n",
        "    return val if isinstance(val, list) else [val]\n",
        "\n",
        "def all_el_unique(arr):\n",
        "    return len(set(arr)) == len(arr)\n",
        "\n",
        "@contextmanager\n",
        "def multi_context(*cms):\n",
        "    with ExitStack() as stack:\n",
        "        yield [stack.enter_context(cls) for cls in cms]\n",
        "\n",
        "def count_intersect(x, y):\n",
        "    # returns an array that shows how many times an element in x is contained in tensor y\n",
        "    return np.sum(rearrange(x, 'i -> i 1') == rearrange(y, 'j -> 1 j'), axis = -1)\n",
        "\n",
        "def check_shape(tensor, pattern, **kwargs):\n",
        "    return rearrange(tensor, f\"{pattern} -> {pattern}\", **kwargs)\n",
        "\n",
        "# a wrapper around faiss IndexIVFFlat\n",
        "# taking care of expiring old keys automagically\n",
        "\n",
        "class KNN():\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        max_num_entries,\n",
        "        cap_num_entries = False,\n",
        "        M = 15,\n",
        "        keep_stats = False\n",
        "    ):\n",
        "        index = faiss.IndexHNSWFlat(dim, M, faiss.METRIC_INNER_PRODUCT)\n",
        "        self.index = index\n",
        "        self.max_num_entries = max_num_entries\n",
        "        self.cap_num_entries = cap_num_entries\n",
        "        self.is_trained = False\n",
        "        self.keep_stats = keep_stats\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'index'):\n",
        "            del self.index\n",
        "\n",
        "    def reset(self):\n",
        "        self.ids = np.empty((0,), dtype = np.int32)\n",
        "\n",
        "        if self.keep_stats:\n",
        "            self.hits = np.empty((0,), dtype = np.int32)\n",
        "            self.age_num_iterations = np.empty((0,), dtype = np.int32)\n",
        "            self.ages_since_last_hit = np.empty((0,), dtype = np.int32)\n",
        "\n",
        "        self.index.reset()\n",
        "        self.is_trained = False\n",
        "\n",
        "    def train(self, x):\n",
        "        self.index.train(x)\n",
        "        self.is_trained = True\n",
        "\n",
        "    def add(self, x, ids):\n",
        "        if not self.is_trained:\n",
        "            self.train(x)\n",
        "\n",
        "        self.ids = np.concatenate((ids, self.ids))\n",
        "\n",
        "        if self.keep_stats:\n",
        "            self.hits = np.concatenate((np.zeros_like(ids), self.hits))\n",
        "            self.age_num_iterations = np.concatenate((np.zeros_like(ids), self.age_num_iterations))\n",
        "            self.ages_since_last_hit = np.concatenate((np.zeros_like(ids), self.ages_since_last_hit))\n",
        "\n",
        "        if self.cap_num_entries and len(self.ids) > self.max_num_entries:\n",
        "            self.reset()\n",
        "\n",
        "        return self.index.add(x)\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        x,\n",
        "        topk,\n",
        "        nprobe = 8,\n",
        "        return_distances = False,\n",
        "        increment_hits = False,\n",
        "        increment_age = True\n",
        "    ):\n",
        "        if not self.is_trained:\n",
        "            return np.full((x.shape[0], topk), -1)\n",
        "\n",
        "        distances, indices = self.index.search(x, k = topk)\n",
        "\n",
        "        if increment_hits and self.keep_stats:\n",
        "            hits = count_intersect(self.ids, rearrange(indices, '... -> (...)'))\n",
        "            self.hits += hits\n",
        "\n",
        "            self.ages_since_last_hit += 1\n",
        "            self.ages_since_last_hit *= (hits == 0)\n",
        "\n",
        "        if increment_age and self.keep_stats:\n",
        "            self.age_num_iterations += 1\n",
        "\n",
        "        if return_distances:\n",
        "            return indices, distances\n",
        "\n",
        "        return indices\n",
        "\n",
        "# KNN memory layer, where one can store key / value memories\n",
        "# can automatically take care of a collection of faiss indices (across batch dimension)\n",
        "\n",
        "class KNNMemory():\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        max_memories = 16000,\n",
        "        num_indices = 1,\n",
        "        memmap_filename = './knn.memory.memmap',\n",
        "        multiprocessing = True\n",
        "    ):\n",
        "        self.dim = dim\n",
        "        self.num_indices = num_indices\n",
        "        self.scoped_indices = list(range(num_indices))\n",
        "\n",
        "        self.max_memories = max_memories\n",
        "        self.shape = (num_indices, max_memories, 2, dim)\n",
        "        self.db_offsets = np.zeros(num_indices, dtype = np.int32)\n",
        "\n",
        "        self.db = np.memmap(memmap_filename, mode = 'w+', dtype = np.float32, shape = self.shape)\n",
        "        self.knns = [KNN(dim = dim, max_num_entries = max_memories, cap_num_entries = True) for _ in range(num_indices)]\n",
        "    \n",
        "        self.n_jobs = cpu_count() if multiprocessing else 1\n",
        "\n",
        "    def set_scoped_indices(self, indices):\n",
        "        indices = list(indices)\n",
        "        assert all_el_unique(indices), f'all scoped batch indices must be unique, received: {indices}'\n",
        "        assert all([0 <= i < self.num_indices for i in indices]), f'each batch index must be between 0 and less than {self.num_indices}: received {indices}'\n",
        "        self.scoped_indices = indices\n",
        "\n",
        "    @contextmanager\n",
        "    def at_batch_indices(self, indices):\n",
        "        prev_indices = self.scoped_indices\n",
        "        self.set_scoped_indices(indices)\n",
        "        yield self\n",
        "        self.set_scoped_indices(prev_indices)\n",
        "\n",
        "    def clear(self, batch_indices = None):\n",
        "        if not exists(batch_indices):\n",
        "            batch_indices = list(range(self.num_indices))\n",
        "\n",
        "        batch_indices = cast_list(batch_indices)\n",
        "\n",
        "        for index in batch_indices:\n",
        "            knn = self.knns[index]\n",
        "            knn.reset()\n",
        "\n",
        "        self.db_offsets[batch_indices] = 0\n",
        "\n",
        "    def add(self, memories):\n",
        "        check_shape(memories, 'b n kv d', d = self.dim, kv = 2, b = len(self.scoped_indices))\n",
        "\n",
        "        memories = memories.detach().cpu().numpy()\n",
        "        memories = memories[:, -self.max_memories:]\n",
        "        num_memories = memories.shape[1]\n",
        "\n",
        "        knn_insert_ids = np.arange(num_memories)\n",
        "\n",
        "        keys = np.ascontiguousarray(memories[..., 0, :])\n",
        "        knns = [self.knns[i] for i in self.scoped_indices]\n",
        "        db_offsets = [self.db_offsets[i] for i in self.scoped_indices]\n",
        "\n",
        "        # use joblib to insert new key / value memories into faiss index\n",
        "\n",
        "        @delayed\n",
        "        def knn_add(knn, key, db_offset):\n",
        "            knn.add(key, ids = knn_insert_ids + db_offset)\n",
        "            return knn\n",
        "\n",
        "        updated_knns = Parallel(n_jobs = self.n_jobs)(knn_add(*args) for args in zip(knns, keys, db_offsets))\n",
        "        for knn_idx, scoped_idx in enumerate(self.scoped_indices):\n",
        "            self.knns[scoped_idx] = updated_knns[knn_idx]\n",
        "\n",
        "        # add the new memories to the memmap \"database\"\n",
        "\n",
        "        add_indices = (rearrange(np.arange(num_memories), 'j -> 1 j') + rearrange(self.db_offsets[list(self.scoped_indices)], 'i -> i 1')) % self.max_memories\n",
        "        self.db[rearrange(np.array(self.scoped_indices), 'i -> i 1'), add_indices] = memories\n",
        "        self.db.flush()\n",
        "\n",
        "        self.db_offsets += num_memories\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        queries,\n",
        "        topk,\n",
        "        nprobe = 8,\n",
        "        increment_hits = True,\n",
        "        increment_age = True\n",
        "    ):\n",
        "        check_shape(queries, 'b ... d', d = self.dim, b = len(self.scoped_indices))\n",
        "        queries, ps = pack([queries], 'b * d')\n",
        "\n",
        "        device = queries.device\n",
        "        queries = queries.detach().cpu().numpy()\n",
        "\n",
        "        all_masks = []\n",
        "        all_key_values = []\n",
        "\n",
        "        knns = [self.knns[i] for i in self.scoped_indices]\n",
        "\n",
        "        # parallelize faiss search\n",
        "\n",
        "        @delayed\n",
        "        def knn_search(knn, query):\n",
        "            return knn.search(query, topk, nprobe, increment_hits = increment_hits, increment_age = increment_age)\n",
        "\n",
        "        fetched_indices = Parallel(n_jobs = self.n_jobs)(knn_search(*args) for args in zip(knns, queries))\n",
        "\n",
        "        # get all the memory key / values from memmap 'database'\n",
        "        # todo - remove for loop below\n",
        "\n",
        "        for batch_index, indices in zip(self.scoped_indices, fetched_indices):\n",
        "            mask = indices !=  -1\n",
        "            db_indices = np.where(mask, indices, 0)\n",
        "\n",
        "            all_masks.append(torch.from_numpy(mask))\n",
        "\n",
        "            key_values = self.db[batch_index, db_indices % self.max_memories]\n",
        "            all_key_values.append(torch.from_numpy(key_values))\n",
        "\n",
        "        all_masks = torch.stack(all_masks)\n",
        "        all_key_values = torch.stack(all_key_values)\n",
        "        all_key_values = all_key_values.masked_fill(~rearrange(all_masks, '... -> ... 1 1'), 0.)\n",
        "\n",
        "        all_key_values, = unpack(all_key_values, ps, 'b * n kv d')\n",
        "        all_masks, = unpack(all_masks, ps, 'b * n')\n",
        "\n",
        "        return all_key_values.to(device), all_masks.to(device)\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'knns'):\n",
        "            for knn in self.knns:\n",
        "                del knn\n",
        "        del self.db\n",
        "\n",
        "# extends list with some extra methods for collections of KNN memories\n",
        "\n",
        "class KNNMemoryList(list):\n",
        "    def cleanup(self):\n",
        "        for memory in self:\n",
        "            del memory\n",
        "\n",
        "    @classmethod\n",
        "    def create_memories(\n",
        "        self,\n",
        "        *,\n",
        "        batch_size,\n",
        "        num_memory_layers,\n",
        "        memories_directory = DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY\n",
        "    ):\n",
        "        memories_path = Path(memories_directory)\n",
        "        memories_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "        def inner(*args, **kwargs):\n",
        "            return self([KNNMemory(*args, num_indices = batch_size, memmap_filename = str(memories_path / f'knn.memory.layer.{ind + 1}.memmap'), **kwargs) for ind in range(num_memory_layers)])\n",
        "        return inner\n",
        "\n",
        "    @contextmanager\n",
        "    def at_batch_indices(\n",
        "        self,\n",
        "        indices\n",
        "    ):\n",
        "        knn_batch_indices_contexts = [memory.at_batch_indices(indices) for memory in self]\n",
        "        with multi_context(*knn_batch_indices_contexts):\n",
        "            yield\n",
        "\n",
        "    def clear_memory(\n",
        "        self,\n",
        "        batch_indices = None,\n",
        "        memory_indices = None\n",
        "    ):\n",
        "        memory_indices = default(memory_indices, tuple(range(len(self))))\n",
        "\n",
        "        for memory_index in memory_indices:\n",
        "            memory = self[memory_index]\n",
        "            memory.clear(batch_indices)"
      ],
      "metadata": {
        "id": "E5Wk2XJSLr-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memorizing transformers"
      ],
      "metadata": {
        "id": "tWJS8R3fL7RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, sqrt_q, device):\n",
        "    t = torch.matmul(query, key.transpose(-2, -1))/sqrt_q\n",
        "    i, j = t.shape[-2:]\n",
        "    mask = torch.ones((i, j), dtype = torch.bool, device = device).triu(j - i + 1)\n",
        "    return torch.matmul(F.softmax(t.masked_fill_(mask, -1e-9), dim = -1), value)\n",
        "\n",
        "def KNNattention(query, key, value, sqrt_q, mask):\n",
        "    t = torch.einsum('b h i d, b h i j d -> b h i j', query, key)/sqrt_q\n",
        "    return torch.einsum('b h i j, b h i j d -> b h i d', F.softmax(t.masked_fill_(mask, -1e-9), dim = -1), value)"
      ],
      "metadata": {
        "id": "ZK8XwbNMp5vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n, d, h, batch_size):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d % h == 0\n",
        "    #assume q = v \n",
        "    self.q = d // h\n",
        "    self.sqrt_q = sqrt(self.q)\n",
        "    self.h = h\n",
        "    self.batch_size = batch_size\n",
        "    self.W_q = nn.Linear(d, d, bias = False) #stack of h matrices of dimension (d, q), one for each head\n",
        "    self.W_k = nn.Linear(d, d, bias = False)\n",
        "    self.W_v = nn.Linear(d, d, bias = False)\n",
        "    self.W_o = nn.Linear(d, d, bias = False)\n",
        "\n",
        "  def forward(self, x, device):\n",
        "    query = self.W_q(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "    key = self.W_k(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "    value = self.W_v(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "    new_memories = torch.stack((key, value), dim = -2).detach()\n",
        "    attention_value = attention(query, key, value, self.sqrt_q, device)\n",
        "    return self.W_o(attention_value.transpose(1, 2).contiguous().view(self.batch_size, -1, self.h*self.q)), new_memories"
      ],
      "metadata": {
        "id": "AwoLII4LMvUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNAttention(nn.Module):\n",
        "   def __init__(self, n, d, h, num_retrieved_memories, batch_size):\n",
        "      super(KNNAttention, self).__init__()\n",
        "      assert d % h == 0\n",
        "      #assume q = v \n",
        "      self.q = d // h\n",
        "      self.sqrt_q = sqrt(self.q)\n",
        "      self.h = h\n",
        "      self.W_q = nn.Linear(d, d, bias = False)\n",
        "      self.W_k = nn.Linear(d, d, bias = False)\n",
        "      self.W_v = nn.Linear(d, d, bias = False)\n",
        "      self.W_o = nn.Linear(d, d, bias = False)\n",
        "      self.b_g = nn.Parameter(torch.randn((h,))) #one for each head\n",
        "      self.num_retrieved_memories = num_retrieved_memories\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "   def forward(self, x, knn_memory, device):\n",
        "      # calculate local attention \n",
        "      query = self.W_q(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "      key = self.W_k(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "      value = self.W_v(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "      local_attention = attention(query, key, value, self.sqrt_q, device)\n",
        "\n",
        "      # calculate knn attention over memory\n",
        "      mem_kv, mem_mask = knn_memory[0].search(query, self.num_retrieved_memories)\n",
        "      mem_key, mem_value = mem_kv.unbind(dim = -2)\n",
        "      knn_attention = KNNattention(query, mem_key, mem_value, self.sqrt_q, ~mem_mask)\n",
        "\n",
        "      # memory to be stored\n",
        "      print(f\"mem_key: {mem_key.shape} mem_value: {mem_value.shape}\")\n",
        "      print(f\"key: {key.shape} value: {value.shape}\")\n",
        "      new_kv_memories = torch.stack((key, value), dim = -2).view(self.batch_size, -1, 2, self.q).detach()\n",
        "\n",
        "      print(f\"new_kv_memories: {new_kv_memories.shape}\")\n",
        "\n",
        "      # add to knn memory\n",
        "      if new_kv_memories.numel() > 0:\n",
        "        knn_memory[0].add(new_kv_memories)\n",
        "\n",
        "      # combining local and memory\n",
        "      print(f\"self.b_g and g shape: {self.b_g.shape}\")\n",
        "      g = F.sigmoid(self.b_g)\n",
        "      print(f\"V_m: {knn_attention.shape}\")\n",
        "      print(f\"V_c: {local_attention.shape}\")\n",
        "      final_attention = torch.einsum('b h n d, h -> b h n d', knn_attention, g) + \\\n",
        "                        torch.einsum('b h n d, h -> b h n d', local_attention, (1 - g))\n",
        "      \n",
        "      return self.W_o(final_attention.transpose(1, 2).contiguous().view(self.batch_size, -1, self.h*self.q)), new_kv_memories"
      ],
      "metadata": {
        "id": "921DW0jjMyWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubLayer(nn.Module):\n",
        "  def __init__(self, d, dropout, hidden_size):\n",
        "    super(SubLayer, self).__init__()\n",
        "    self.norm = nn.LayerNorm(d)\n",
        "    self.mlp = nn.Sequential(nn.Linear(d, hidden_size, bias = True), \n",
        "                             nn.ReLU(),\n",
        "                             nn.Dropout(dropout),\n",
        "                             nn.Linear(hidden_size, d, bias = True))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.mlp(self.norm(x)) #residual connection and normalization"
      ],
      "metadata": {
        "id": "M2Tjkv0gbsmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    # Compute the positional encodings once in log space.\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                          -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    # print(x.shape)\n",
        "    # print(self.pe[:, :x.size(1)].shape)\n",
        "    return x + Variable(self.pe[:, :x.size(1)], requires_grad=False)"
      ],
      "metadata": {
        "id": "9U77GkT0cT4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faiss.swigfaiss import knn_inner_product\n",
        "class MemorizingTransformer(nn.Module):\n",
        "    def __init__(\n",
        "          self,\n",
        "          num_tokens,\n",
        "          d,\n",
        "          heads = 8,\n",
        "          depth = 4,\n",
        "          knn_attn_idx = 2,\n",
        "          attn_dropout = 0.,\n",
        "          hidden_size = 1000,\n",
        "          dropout = 0.3,\n",
        "          max_knn_memories = 1000,\n",
        "          num_retrieved_memories = 8,\n",
        "          knn_memories_directory = DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY,\n",
        "          knn_memory_multiprocessing = False,\n",
        "          batch_size = 16\n",
        "      ):\n",
        "          # asserts\n",
        "          assert d % heads == 0\n",
        "          assert knn_attn_idx < depth\n",
        "\n",
        "          super(MemorizingTransformer, self).__init__()\n",
        "          self.token_emb = nn.Embedding(num_tokens, d)\n",
        "          self.positional_emb = PositionalEncoding(d, max_len = 5000)\n",
        "          self.dim_head = d // heads\n",
        "          self.d = d\n",
        "          self.heads = heads\n",
        "          self.knn_attn_idx = knn_attn_idx\n",
        "          self.depth = depth\n",
        "          self.attn_dropout = attn_dropout\n",
        "          self.hidden_size = hidden_size\n",
        "          self.dropout = dropout\n",
        "          self.max_knn_memories = max_knn_memories\n",
        "          self.num_retrieved_memories = num_retrieved_memories\n",
        "          self.knn_memories_directory = knn_memories_directory\n",
        "          self.knn_memory_multiprocessing =knn_memory_multiprocessing\n",
        "          self.batch_size = batch_size\n",
        "\n",
        "          self.layers = nn.ModuleList([])\n",
        "          for idx in range(depth):\n",
        "              attn = KNNAttention(num_tokens, d, heads, num_retrieved_memories, self.batch_size) \\\n",
        "                  if idx == knn_attn_idx else MultiHeadAttention(num_tokens, d, heads, self.batch_size)\n",
        "\n",
        "              self.layers.append(nn.ModuleList([\n",
        "                  attn,\n",
        "                  SubLayer(d, dropout, hidden_size)\n",
        "              ]))\n",
        "\n",
        "          self.to_out = nn.Sequential(\n",
        "               nn.LayerNorm(d),\n",
        "               nn.Linear(d, num_tokens)\n",
        "          )\n",
        "\n",
        "          # knn memories init\n",
        "\n",
        "          self.knn_mem_kwargs = dict(\n",
        "              dim = self.dim_head,\n",
        "              max_memories = self.max_knn_memories,\n",
        "              multiprocessing = knn_memory_multiprocessing\n",
        "          )\n",
        "          \n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        knn_memory\n",
        "    ):\n",
        "        batch_size, seq_len, *_, device = *x.shape, x.device\n",
        "        x = self.token_emb(x)\n",
        "        x = self.positional_emb(x)\n",
        "\n",
        "        for idx, (attn, sub_l) in enumerate(self.layers):\n",
        "            \n",
        "            #attention\n",
        "\n",
        "            print()\n",
        "            print(self.knn_attn_idx == idx)\n",
        "            print(f\"Before layer {x.shape}\")  \n",
        "\n",
        "            x, mem = attn(x, knn_memory, device) if self.knn_attn_idx == idx else attn(x, device)\n",
        "\n",
        "            print(f\"After layer {x.shape}\")  \n",
        "      \n",
        "            # feedforward\n",
        "\n",
        "            x = sub_l(x)\n",
        "\n",
        "            print(f\"After sub_layer {x.shape}\")  \n",
        "\n",
        "        return self.to_out(x).transpose(1, 2)\n",
        "\n",
        "    \n",
        "    def create_knn_memories(\n",
        "          self,\n",
        "          *,\n",
        "          batch_size\n",
        "      ):  \n",
        "          return KNNMemoryList.create_memories(\n",
        "              batch_size = batch_size,\n",
        "              num_memory_layers = 1,\n",
        "              memories_directory = self.knn_memories_directory\n",
        "          )(**self.knn_mem_kwargs)\n",
        "      \n",
        "    @contextmanager\n",
        "    def knn_memories_context(\n",
        "        self,\n",
        "        **kwargs\n",
        "    ):\n",
        "        knn_dir = Path(self.knn_memories_directory)\n",
        "        knn_dir.mkdir(exist_ok = True, parents = True)\n",
        "        lock = FileLock(str(knn_dir / 'mutex'))\n",
        "\n",
        "        with lock:\n",
        "            knn_memories = self.create_knn_memories(**kwargs)\n",
        "            yield knn_memories\n",
        "            knn_memories.cleanup()\n",
        "\n",
        "    def clear_memory(self, x, token_id):\n",
        "        \"\"\" clears the KNN memories based on if the batch row contains the specified token id \"\"\"\n",
        "        \"\"\" for auto-clearing KNN memories based on start and end of strings \"\"\"\n",
        "\n",
        "        clear_memory = (x == token_id).any(dim = -1)\n",
        "        batch_indices, _ = clear_memory.nonzero(as_tuple = True)\n",
        "        batch_indices_to_clear = batch_indices.tolist()\n",
        "\n",
        "        if len(batch_indices_to_clear) == 0:\n",
        "            return\n",
        "\n",
        "        knn_memories.clear_memory(batch_indices_to_clear)"
      ],
      "metadata": {
        "id": "F5uqCzrVL569"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ihBnTrPhaiDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "\n",
        "NUM_BATCHES = int(1e5)\n",
        "BATCH_SIZE = 16\n",
        "SEQ_LEN = 512\n",
        "SEGMENTS = 5\n",
        "HEADS = 8\n",
        "DIM_HEAD = SEQ_LEN // HEADS\n",
        "\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_GRAD_CLIP_NORM = 0.5\n",
        "\n",
        "EVAL_EVERY = 20\n",
        "GENERATE_EVERY  = 500\n",
        "GENERATE_LENGTH = 512\n",
        "CHECKPOINT = 10"
      ],
      "metadata": {
        "id": "ncWOJOkFanTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helpers\n",
        "\n",
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "            yield data\n",
        "\n",
        "def decode_token(token):\n",
        "    return str(chr(max(32, token)))\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    return ''.join(list(map(decode_token, tokens)))\n",
        "\n",
        "model = MemorizingTransformer(\n",
        "    num_tokens = 256,\n",
        "    d = SEQ_LEN,\n",
        "    heads = HEADS,\n",
        "    batch_size = BATCH_SIZE\n",
        ").cuda()\n",
        "\n",
        "# prepare enwik8 data\n",
        "\"\"\"\n",
        "#Lorenzo\n",
        "with gzip.open('/content/drive/MyDrive/Secondo Anno/Neural Networks/project/enwik8.gz') as file:\n",
        "    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
        "    trX, vaX = np.split(X, [int(90e6)])\n",
        "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
        "\"\"\"\n",
        "\n",
        "#Luigi\n",
        "with gzip.open('/content/drive/MyDrive/Colab Notebooks/enwik8.gz') as file:\n",
        "    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n",
        "    trX, vaX = np.split(X, [int(90e6)])\n",
        "    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n",
        "\n",
        "class TextSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
        "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
        "        return full_seq.cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.seq_len\n",
        "\n",
        "# dataset and dataloader\n",
        "\n",
        "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
        "train_loader  = DataLoader(train_dataset, batch_size = BATCH_SIZE, drop_last = True)\n",
        "test_dataset = TextSamplerDataset(data_val, SEQ_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, drop_last = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gVvmKCm3J4A",
        "outputId": "72ff63b4-5169-4aeb-d76d-00a10a5ed740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-62e38749ac5b>:32: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next(iter(train_loader))"
      ],
      "metadata": {
        "id": "FFVzjlKX3kuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_string(a):\n",
        "  seq = \"\"\n",
        "  for word in a:\n",
        "    for letter in word:\n",
        "      seq += chr(letter)\n",
        "    seq += \" \"\n",
        "  return seq"
      ],
      "metadata": {
        "id": "e6Apbm_X-ceS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# training\n",
        "\n",
        "for i, data in enumerate(tqdm.tqdm(train_loader, desc = 'training')):\n",
        "    model.train()\n",
        "    # data = next(train_loader)\n",
        "\n",
        "    train_loss = 0.\n",
        "    with model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories:\n",
        "        \n",
        "        seq, labels = data[:, :-1], data[:, 1:] #the labels are the same sequences shifted by one\n",
        "\n",
        "        out = model(\n",
        "              seq,\n",
        "              knn_memory = knn_memories\n",
        "        )\n",
        "        loss_item = torch.exp(loss(out, labels)) #perplexity\n",
        "        train_loss += loss_item\n",
        "        loss_item.backward() \n",
        "\n",
        "    print(f'training loss: {train_loss}')\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if i % EVAL_EVERY == 0:\n",
        "        model.eval()\n",
        "      \n",
        "        test_data = None\n",
        "        for test_data in test_loader:\n",
        "          break\n",
        "\n",
        "        test_loss = 0.\n",
        "\n",
        "        with torch.no_grad(), model.knn_memories_context(batch_size = BATCH_SIZE) as knn_memories: \n",
        "            seq, labels = data[:, :-1], data[:, 1:]\n",
        "            \n",
        "            out = model(\n",
        "              seq,\n",
        "              knn_memory = knn_memories\n",
        "            )\n",
        "\n",
        "            loss_item = loss(out, labels)\n",
        "            test_loss +=  loss_item\n",
        "\n",
        "        print(f'valid loss: {test_loss}')\n",
        "    \n",
        "    if i % CHECKPOINT == 0:\n",
        "      torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "      }, 'model_optimizer.pt')"
      ],
      "metadata": {
        "id": "d00bJntK2Fyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cef12ba-f251-4f24-bacf-5aa078a33219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training:   0%|          | 0/10986 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "', the cloned embryo is implanted in a woman's uterus. This should develop into a normal baby, its only distinction being that it would be almost genetically identical to the DNA donor. Scientific knowledge of normal and abnormal development could also be found.\t \n",
            "\n",
            "'''[[Therapeutic cloning]]''' could be used to provide replacement organs or tissue for people who have had theirs damaged. The cloned embryo would contain DNA taken from the transplant patient. After nuclear transfer, the cell would divide to fo \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ", the cloned embryo is implanted in a woman's uterus. This should develop into a normal baby, its only distinction being that it would be almost genetically identical to the DNA donor. Scientific knowledge of normal and abnormal development could also be found.\t \n",
            "\n",
            "'''[[Therapeutic cloning]]''' could be used to provide replacement organs or tissue for people who have had theirs damaged. The cloned embryo would contain DNA taken from the transplant patient. After nuclear transfer, the cell would divide to for \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}