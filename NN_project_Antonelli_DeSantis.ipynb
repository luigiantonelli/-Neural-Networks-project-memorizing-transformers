{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/NN-project-memorizing-transformers/blob/main/NN_project_Antonelli_DeSantis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we are presenting our implementation of the Memorizing transformer architecture from the paper \"Memorizing Transformers\" by Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins and Christian Szegedy (https://arxiv.org/abs/2203.08913).\n",
        "\n",
        "Memorizing transformers are decoder-only transformers which have the ability to store in a non-differentiable memory the internal representations of past inputs, allowing to combine local attention with a $k$-nearest neighbors search into the memory. In particular, the architecture of these models uses standard transfomer blocks and a special transformer block that uses this modified version of the attention taking also into consideration past information stored in the memory during previous training steps. For simplicity, we will refer to this block as \"memory block\" in the remainder of this notebook.\n",
        "\n",
        "As it is explained in the paper, the memory block is usually put almost at the end of the architecture and the use of more than one of these blocks don't result in better performances. We followed this approach and conduct our experiments by stacking multiple transformer blocks, followed by a memory block and by one last standard transformer block.\n",
        "\n",
        "The task that we are training our models on is language modeling. So the models are trained on text sequences and are evaluated on their ability to predict "
      ],
      "metadata": {
        "id": "ZZGF-YhbMYid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports"
      ],
      "metadata": {
        "id": "-4GElPh7HbB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section there is everything that is needed to run the cells of the notebook."
      ],
      "metadata": {
        "id": "lVJsp0dwMVb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sW8KnU4lPkgI",
        "outputId": "da20dfdd-ae07-42bd-e1ad-d0dc479b7508"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_transformers import BertTokenizer\n",
        "from pytorch_transformers import BertModel"
      ],
      "metadata": {
        "id": "G5xdOnnKPbxi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata --quiet\n",
        "!pip install torchmetrics --quiet\n",
        "!pip install torchtext --quiet\n",
        "!pip install -U spacy --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet"
      ],
      "metadata": {
        "id": "oZJbIamFP23c",
        "outputId": "37d879fb-effd-4bbd-e847-59b92e6c9c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from pathlib import Path\n",
        "from filelock import FileLock\n",
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pickle as pkl\n",
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import spacy\n",
        "from typing import Iterable, List\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchmetrics.text.perplexity import Perplexity\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "metadata": {
        "id": "Kd714QnlGIP-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a4BTaVB5Pxs",
        "outputId": "05ed6dd3-849f-4ae5-c66a-c8c49f7137a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "zAXedSwA5nyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the datasets mentioned in the paper are too big to handle in reasonable time in a Colab notebook, we decided to use WikiText2, a smaller dataset.\n",
        "\n",
        "In this section the dataset is tokenized and then organized into documents. The dataset contains documents from Wikipedia, but the iterator returns them line by line and there are no end-of-sequence tokens at the end of each document that help in distinguishing different documents.\n",
        "In order to overcome this issue we decided to create fictitious documents of 10000 tokens each, so that we could train our models like described in the paper. For the purpose of the memorizing transformer architecture, having small sequences such as lines as training sample swouldn't justify the use of the external memory. In fact, the documents should be divided in subsequences and each long document should be fed into the memorizing transformer sequentially from start to end without doing any shuffling.\n",
        "Obviously, this is not optimal and not the same as having a dataset like the ones mentioned in the paper, but it's enough to conduct some experiments like we did.\n",
        "\n",
        "We tried two different alternatives for tokenization. The first one consists in iterating over the lines in the dataset and building a vocabulary and then tokenize the dataset by using the vocabulary. The second approach instead consists in tokenizing directly the dataset with the help of the Bert tokenizer.\n",
        "The dataset is then organized in a long tensor containing all the tokens and then organized into documents. With the first approach, the transformer that would use the layer \"nn.Embedding\" from Pytorch to create the embeddings starting from a minibatch of subsequences of tokenized documents. On the other hand, with the second approach the transformer would use the Bert model to create the embeddings. The second approach performs better than the first one but of course it's more expensive both in terms of time and in GPU memory usage."
      ],
      "metadata": {
        "id": "WTNyqPAfXWxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = WikiText2(split = ('train', 'test'))"
      ],
      "metadata": {
        "id": "jag9JLsDbyOB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from itertools import chain\n",
        "data_iter = chain(train_iter, test_iter)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J4Lo0Rsz7fT-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed02b078-ad38-4457-c442-eacde8055683"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom itertools import chain\\ndata_iter = chain(train_iter, test_iter)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "token_transform = get_tokenizer('spacy', language = 'en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data) -> List[str]:\n",
        "    for line in data:\n",
        "        yield token_transform(line)\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "vocabulary_ = build_vocab_from_iterator(yield_tokens(data_iter), min_freq = 1, specials = special_symbols, special_first = True)\n",
        "vocabulary_.set_default_index(UNK_IDX)\n",
        "vocabulary_.__len__()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uvcwcyno7i6V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c98cde6f-05b5-4870-8a1f-c8f8058261d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntoken_transform = get_tokenizer('spacy', language = 'en_core_web_sm')\\n\\ndef yield_tokens(data) -> List[str]:\\n    for line in data:\\n        yield token_transform(line)\\n\\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\\n\\nvocabulary_ = build_vocab_from_iterator(yield_tokens(data_iter), min_freq = 1, specials = special_symbols, special_first = True)\\nvocabulary_.set_default_index(UNK_IDX)\\nvocabulary_.__len__()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def preprocessing_(dataset):\n",
        "  new_ds = torch.tensor([], dtype = torch.int32)\n",
        "  for line in dataset:\n",
        "    tokenized_line = torch.tensor([vocabulary_[token] for token in token_transform(line)])\n",
        "    new_ds = torch.cat((new_ds, tokenized_line))\n",
        "  return new_ds\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0JbiwRcz8G7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e84c162c-5f92-42d6-a8bc-fa1549f356f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef preprocessing_(dataset):\\n  new_ds = torch.tensor([], dtype = torch.int32)\\n  for line in dataset:\\n    tokenized_line = torch.tensor([vocabulary_[token] for token in token_transform(line)])\\n    new_ds = torch.cat((new_ds, tokenized_line))\\n  return new_ds\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train_ds = preprocessing_(train_iter)\n",
        "test_ds = preprocessing_(test_iter)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R29fLy2G8fIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d7b227d-f47e-41b1-d920-227e6c1f7247"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_ds = preprocessing_(train_iter)\\ntest_ds = preprocessing_(test_iter)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zlNANpHQiOt",
        "outputId": "ace5f2b3-4586-419f-fa4a-d1e5d1019b6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2549311.57B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(dataset):\n",
        "  new_ds = []\n",
        "  for line in dataset:\n",
        "    tokenized_line = tokenizer.tokenize(line)\n",
        "    new_ds.append(tokenized_line)\n",
        "  return new_ds"
      ],
      "metadata": {
        "id": "RnfQlE72RQCR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = preprocessing(train_iter)\n",
        "test_ds = preprocessing(test_iter)"
      ],
      "metadata": {
        "id": "t4ZDCYlDRei8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tokens_to_idxs(dataset):\n",
        "  new_ds = torch.tensor([], dtype = torch.int32)\n",
        "  for line in dataset:\n",
        "    tokenized_line = torch.tensor(tokenizer.convert_tokens_to_ids(line))    \n",
        "    new_ds = torch.cat((new_ds, tokenized_line))\n",
        "  return new_ds"
      ],
      "metadata": {
        "id": "_wQHPc5URusJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = convert_tokens_to_idxs(train_ds)\n",
        "test_ds = convert_tokens_to_idxs(test_ds)"
      ],
      "metadata": {
        "id": "VjlekiRPSahL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCEOkwkgTQiS",
        "outputId": "6b202fa3-cc29-4e4e-dfc2-67b16cc2f211"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2405592])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = 30522"
      ],
      "metadata": {
        "id": "juDl2xbjdzEF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size = 10000, shuffle = False, drop_last = True)\n",
        "print(len(train_loader))\n",
        "train_ds = torch.zeros((len(train_loader), 10000), dtype = torch.int32)\n",
        "for i, document in enumerate(train_loader):\n",
        "  train_ds[i] = document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zCX45xyL955",
        "outputId": "39bc3e5d-6559-4622-9943-f529a6a0ed98"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_ds, batch_size = 10000, shuffle = False, drop_last = True)\n",
        "\n",
        "test_ds = torch.zeros((len(test_loader), 10000), dtype = torch.int32)\n",
        "for i, document in enumerate(test_loader):\n",
        "  test_ds[i] = document"
      ],
      "metadata": {
        "id": "KlSGhT9-dCDt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGcfKRdsP98_",
        "outputId": "dc64aeec-0502-40ac-d8bf-02b21f53083d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([240, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds.shape"
      ],
      "metadata": {
        "id": "B0wCMRhZdatc",
        "outputId": "6449f2cd-01cd-4d6c-b18b-153e2c552df3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Memory"
      ],
      "metadata": {
        "id": "4I2ce2jPLzle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the notebook is dedicated to the definition of the external memory that will be used by the memorizing transformer. We were allowed to use this code from https://github.com/lucidrains/memorizing-transformers-pytorch/blob/main/memorizing_transformers_pytorch/knn_memory.py. \n",
        "\n",
        "Right before training we will instanciate an object of the class KNNMemory defined in this section that will be one of the inputs of the memory block of our model. With the building blocks provided in the next sections it's possible to create a memorizing transformer with more memory blocks. In that case, it would be necessary to create one KNNMemory object for each of these blocks and modify the forward method of our class."
      ],
      "metadata": {
        "id": "8pYePr4DhW65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QppURbZ0KL0f",
        "outputId": "e051eb71-9507-4dfc-ac64-e6d9d2415391"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krC3-klsLPVa",
        "outputId": "ef4ff487-4a26-4f11-9cdb-8ebbffad9806"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import per la knn memory\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from functools import wraps\n",
        "\n",
        "from contextlib import ExitStack, contextmanager\n",
        "\n",
        "from einops import rearrange, pack, unpack\n",
        "\n",
        "# multiprocessing\n",
        "\n",
        "from joblib import Parallel, delayed, cpu_count"
      ],
      "metadata": {
        "id": "KeHkdSn0KBSP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FAISS_INDEX_GPU_ID = int(os.getenv('FAISS_INDEX_GPU_ID', 0))\n",
        "\n",
        "DEFAULT_KNN_MEMORY_MEMMAP_DIRECTORY = './.tmp/knn.memories'\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_list(val):\n",
        "    return val if isinstance(val, list) else [val]\n",
        "\n",
        "def all_el_unique(arr):\n",
        "    return len(set(arr)) == len(arr)\n",
        "\n",
        "@contextmanager\n",
        "def multi_context(*cms):\n",
        "    with ExitStack() as stack:\n",
        "        yield [stack.enter_context(cls) for cls in cms]\n",
        "\n",
        "def count_intersect(x, y):\n",
        "    # returns an array that shows how many times an element in x is contained in tensor y\n",
        "    return np.sum(rearrange(x, 'i -> i 1') == rearrange(y, 'j -> 1 j'), axis = -1)\n",
        "\n",
        "def check_shape(tensor, pattern, **kwargs):\n",
        "    return rearrange(tensor, f\"{pattern} -> {pattern}\", **kwargs)\n",
        "\n",
        "# a wrapper around faiss IndexIVFFlat\n",
        "# taking care of expiring old keys automagically\n",
        "\n",
        "class KNN():\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        max_num_entries,\n",
        "        cap_num_entries = False,\n",
        "        M = 15,\n",
        "        keep_stats = False\n",
        "    ):\n",
        "        index = faiss.IndexHNSWFlat(dim, M, faiss.METRIC_INNER_PRODUCT)\n",
        "        self.index = index\n",
        "        self.max_num_entries = max_num_entries\n",
        "        self.cap_num_entries = cap_num_entries\n",
        "        self.is_trained = False\n",
        "        self.keep_stats = keep_stats\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'index'):\n",
        "            del self.index\n",
        "\n",
        "    def reset(self):\n",
        "        self.ids = np.empty((0,), dtype = np.int32)\n",
        "\n",
        "        if self.keep_stats:\n",
        "            self.hits = np.empty((0,), dtype = np.int32)\n",
        "            self.age_num_iterations = np.empty((0,), dtype = np.int32)\n",
        "            self.ages_since_last_hit = np.empty((0,), dtype = np.int32)\n",
        "\n",
        "        self.index.reset()\n",
        "        self.is_trained = False\n",
        "\n",
        "    def train(self, x):\n",
        "        self.index.train(x)\n",
        "        self.is_trained = True\n",
        "\n",
        "    def add(self, x, ids):\n",
        "        if not self.is_trained:\n",
        "            self.train(x)\n",
        "\n",
        "        self.ids = np.concatenate((ids, self.ids))\n",
        "\n",
        "        if self.keep_stats:\n",
        "            self.hits = np.concatenate((np.zeros_like(ids), self.hits))\n",
        "            self.age_num_iterations = np.concatenate((np.zeros_like(ids), self.age_num_iterations))\n",
        "            self.ages_since_last_hit = np.concatenate((np.zeros_like(ids), self.ages_since_last_hit))\n",
        "\n",
        "        if self.cap_num_entries and len(self.ids) > self.max_num_entries:\n",
        "            self.reset()\n",
        "\n",
        "        return self.index.add(x)\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        x,\n",
        "        topk,\n",
        "        nprobe = 8,\n",
        "        return_distances = False,\n",
        "        increment_hits = False,\n",
        "        increment_age = True\n",
        "    ):\n",
        "        if not self.is_trained:\n",
        "            return np.full((x.shape[0], topk), -1)\n",
        "\n",
        "        distances, indices = self.index.search(x, k = topk)\n",
        "\n",
        "        if increment_hits and self.keep_stats:\n",
        "            hits = count_intersect(self.ids, rearrange(indices, '... -> (...)'))\n",
        "            self.hits += hits\n",
        "\n",
        "            self.ages_since_last_hit += 1\n",
        "            self.ages_since_last_hit *= (hits == 0)\n",
        "\n",
        "        if increment_age and self.keep_stats:\n",
        "            self.age_num_iterations += 1\n",
        "\n",
        "        if return_distances:\n",
        "            return indices, distances\n",
        "\n",
        "        return indices\n",
        "\n",
        "# KNN memory layer, where one can store key / value memories\n",
        "# can automatically take care of a collection of faiss indices (across batch dimension)\n",
        "\n",
        "class KNNMemory():\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        max_memories = 16000,\n",
        "        num_indices = 1,\n",
        "        memmap_filename = './knn.memory.memmap',\n",
        "        multiprocessing = True\n",
        "    ):\n",
        "        self.dim = dim\n",
        "        self.num_indices = num_indices\n",
        "        self.scoped_indices = list(range(num_indices))\n",
        "\n",
        "        self.max_memories = max_memories\n",
        "        self.shape = (num_indices, max_memories, 2, dim)\n",
        "        self.db_offsets = np.zeros(num_indices, dtype = np.int32)\n",
        "\n",
        "        self.db = np.memmap(memmap_filename, mode = 'w+', dtype = np.float32, shape = self.shape)\n",
        "        self.knns = [KNN(dim = dim, max_num_entries = max_memories, cap_num_entries = True) for _ in range(num_indices)]\n",
        "    \n",
        "        self.n_jobs = cpu_count() if multiprocessing else 1\n",
        "\n",
        "    def set_scoped_indices(self, indices):\n",
        "        indices = list(indices)\n",
        "        assert all_el_unique(indices), f'all scoped batch indices must be unique, received: {indices}'\n",
        "        assert all([0 <= i < self.num_indices for i in indices]), f'each batch index must be between 0 and less than {self.num_indices}: received {indices}'\n",
        "        self.scoped_indices = indices\n",
        "\n",
        "    @contextmanager\n",
        "    def at_batch_indices(self, indices):\n",
        "        prev_indices = self.scoped_indices\n",
        "        self.set_scoped_indices(indices)\n",
        "        yield self\n",
        "        self.set_scoped_indices(prev_indices)\n",
        "\n",
        "    def clear(self, batch_indices = None):\n",
        "        if not exists(batch_indices):\n",
        "            batch_indices = list(range(self.num_indices))\n",
        "\n",
        "        batch_indices = cast_list(batch_indices)\n",
        "\n",
        "        for index in batch_indices:\n",
        "            knn = self.knns[index]\n",
        "            knn.reset()\n",
        "\n",
        "        self.db_offsets[batch_indices] = 0\n",
        "\n",
        "    def add(self, memories):\n",
        "        check_shape(memories, 'b n kv d', d = self.dim, kv = 2, b = len(self.scoped_indices))\n",
        "\n",
        "        memories = memories.detach().cpu().numpy()\n",
        "        memories = memories[:, -self.max_memories:]\n",
        "        num_memories = memories.shape[1]\n",
        "\n",
        "        knn_insert_ids = np.arange(num_memories)\n",
        "\n",
        "        keys = np.ascontiguousarray(memories[..., 0, :])\n",
        "        knns = [self.knns[i] for i in self.scoped_indices]\n",
        "        db_offsets = [self.db_offsets[i] for i in self.scoped_indices]\n",
        "\n",
        "        # use joblib to insert new key / value memories into faiss index\n",
        "\n",
        "        @delayed\n",
        "        def knn_add(knn, key, db_offset):\n",
        "            knn.add(key, ids = knn_insert_ids + db_offset)\n",
        "            return knn\n",
        "\n",
        "        updated_knns = Parallel(n_jobs = self.n_jobs)(knn_add(*args) for args in zip(knns, keys, db_offsets))\n",
        "        for knn_idx, scoped_idx in enumerate(self.scoped_indices):\n",
        "            self.knns[scoped_idx] = updated_knns[knn_idx]\n",
        "\n",
        "        # add the new memories to the memmap \"database\"\n",
        "\n",
        "        add_indices = (rearrange(np.arange(num_memories), 'j -> 1 j') + rearrange(self.db_offsets[list(self.scoped_indices)], 'i -> i 1')) % self.max_memories\n",
        "        self.db[rearrange(np.array(self.scoped_indices), 'i -> i 1'), add_indices] = memories\n",
        "        self.db.flush()\n",
        "\n",
        "        self.db_offsets += num_memories\n",
        "\n",
        "    def search(\n",
        "        self,\n",
        "        queries,\n",
        "        topk,\n",
        "        nprobe = 8,\n",
        "        increment_hits = True,\n",
        "        increment_age = True\n",
        "    ):\n",
        "        check_shape(queries, 'b ... d', d = self.dim, b = len(self.scoped_indices))\n",
        "        queries, ps = pack([queries], 'b * d')\n",
        "\n",
        "        device = queries.device\n",
        "        queries = queries.detach().cpu().numpy()\n",
        "\n",
        "        all_masks = []\n",
        "        all_key_values = []\n",
        "\n",
        "        knns = [self.knns[i] for i in self.scoped_indices]\n",
        "\n",
        "        # parallelize faiss search\n",
        "\n",
        "        @delayed\n",
        "        def knn_search(knn, query):\n",
        "            return knn.search(query, topk, nprobe, increment_hits = increment_hits, increment_age = increment_age)\n",
        "\n",
        "        fetched_indices = Parallel(n_jobs = self.n_jobs)(knn_search(*args) for args in zip(knns, queries))\n",
        "\n",
        "        # get all the memory key / values from memmap 'database'\n",
        "        # todo - remove for loop below\n",
        "\n",
        "        for batch_index, indices in zip(self.scoped_indices, fetched_indices):\n",
        "            mask = indices !=  -1\n",
        "            db_indices = np.where(mask, indices, 0)\n",
        "\n",
        "            all_masks.append(torch.from_numpy(mask))\n",
        "\n",
        "            key_values = self.db[batch_index, db_indices % self.max_memories]\n",
        "            all_key_values.append(torch.from_numpy(key_values))\n",
        "\n",
        "        all_masks = torch.stack(all_masks)\n",
        "        all_key_values = torch.stack(all_key_values)\n",
        "        all_key_values = all_key_values.masked_fill(~rearrange(all_masks, '... -> ... 1 1'), 0.)\n",
        "\n",
        "        all_key_values, = unpack(all_key_values, ps, 'b * n kv d')\n",
        "        all_masks, = unpack(all_masks, ps, 'b * n')\n",
        "\n",
        "        return all_key_values.to(device), all_masks.to(device)\n",
        "\n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'knns'):\n",
        "            for knn in self.knns:\n",
        "                del knn\n",
        "        del self.db"
      ],
      "metadata": {
        "id": "E5Wk2XJSLr-9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memorizing transformers"
      ],
      "metadata": {
        "id": "tWJS8R3fL7RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, sqrt_q, device, mask = None):\n",
        "    t = torch.matmul(query, key.transpose(-2, -1))/sqrt_q\n",
        "    if mask is not None:\n",
        "      t = t.masked_fill_(mask == 0, -1e-9)\n",
        "    return torch.matmul(F.softmax(t, dim = -1), value)\n",
        "\n",
        "def KNNattention(query, key, value, sqrt_q, mask):\n",
        "    t = torch.einsum('b h i q, b h i j q -> b h i j', query, key)/sqrt_q\n",
        "    return torch.einsum('b h i j, b h i j q -> b h i q', F.softmax(t.masked_fill_(mask, -1e-9), dim = -1), value)"
      ],
      "metadata": {
        "id": "ZK8XwbNMp5vT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d, h, batch_size):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d % h == 0\n",
        "    #assume q = v \n",
        "    self.q = d // h #single head dimension\n",
        "    self.sqrt_q = sqrt(self.q)\n",
        "    self.h = h\n",
        "    self.batch_size = batch_size\n",
        "    self.W_q = nn.Linear(d, d, bias = False) #stack of h matrices of dimension (d, q), one for each head\n",
        "    self.W_k = nn.Linear(d, d, bias = False)\n",
        "    self.W_v = nn.Linear(d, d, bias = False)\n",
        "    self.W_o = nn.Linear(d, d, bias = False)\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    query = self.W_q(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "    key = self.W_k(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "    value = self.W_v(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "    #new_memories = torch.stack((key, value), dim = -2).detach()\n",
        "    attention_value = attention(query, key, value, self.sqrt_q, mask)\n",
        "    return self.W_o(attention_value.transpose(1, 2).contiguous().view(self.batch_size, -1, self.h*self.q))"
      ],
      "metadata": {
        "id": "AwoLII4LMvUK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNAttention(nn.Module):\n",
        "   def __init__(self, d, h, batch_size, num_retrieved_memories):\n",
        "      super(KNNAttention, self).__init__()\n",
        "      assert d % h == 0\n",
        "      #assume q = v \n",
        "      self.q = d // h\n",
        "      self.sqrt_q = sqrt(self.q)\n",
        "      self.h = h\n",
        "      self.W_q = nn.Linear(d, d, bias = False)\n",
        "      self.W_k = nn.Linear(d, d, bias = False)\n",
        "      self.W_v = nn.Linear(d, d, bias = False)\n",
        "      self.W_o = nn.Linear(d, d, bias = False)\n",
        "      self.b_g = nn.Parameter(torch.randn((h,))) #one for each head\n",
        "      self.num_retrieved_memories = num_retrieved_memories\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "   def forward(self, x, mask, knn_memory):\n",
        "      # calculate local attention \n",
        "      query = self.W_q(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "      key = self.W_k(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "      value = self.W_v(x).view(self.batch_size, -1, self.h, self.q).transpose(1, 2)\n",
        "      local_attention = attention(query, key, value, self.sqrt_q, mask)\n",
        "\n",
        "      # calculate knn attention over memory\n",
        "      query = F.normalize(query, dim = -1)\n",
        "      key = F.normalize(key, dim = -1)\n",
        "      mem_kv, mem_mask = knn_memory.search(query, self.num_retrieved_memories)\n",
        "      mem_key, mem_value = mem_kv.unbind(dim = -2)\n",
        "      knn_attention = KNNattention(query, mem_key, mem_value, self.sqrt_q, ~mem_mask)\n",
        "\n",
        "      # memory to be stored\n",
        "      new_kv_memories = torch.stack((key, value), dim = -2).view(self.batch_size, -1, 2, self.q).detach()\n",
        "\n",
        "      # add to knn memory\n",
        "      if new_kv_memories.numel() > 0:\n",
        "        knn_memory.add(new_kv_memories)\n",
        "\n",
        "      # combining local and memory\n",
        "      g = torch.sigmoid(self.b_g)\n",
        "      final_attention = torch.einsum('b h n q, h -> b h n q', knn_attention, g) + \\\n",
        "                        torch.einsum('b h n q, h -> b h n q', local_attention, (1 - g))\n",
        "      \n",
        "      return self.W_o(final_attention.transpose(1, 2).contiguous().view(self.batch_size, -1, self.h*self.q))"
      ],
      "metadata": {
        "id": "921DW0jjMyWx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    # Compute the positional encodings once in log space.\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                          -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return x + Variable(self.pe[:, :x.size(1)], requires_grad=False)"
      ],
      "metadata": {
        "id": "9U77GkT0cT4u"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d, h, batch_size, hidden_size, dropout, is_mem = False, num_retrieved_memories = 32):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.d = d\n",
        "    self.h = h\n",
        "    self.batch_size = batch_size\n",
        "    self.attention = MultiHeadAttention(d, h, batch_size) if not is_mem else KNNAttention(d, h, batch_size, num_retrieved_memories)\n",
        "    self.norm1 = nn.LayerNorm(d)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.norm2 = nn.LayerNorm(d)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.ff = nn.Sequential(nn.Linear(d, hidden_size, bias = True), \n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(dropout),\n",
        "                            nn.Linear(hidden_size, d, bias = True))\n",
        "  def forward(self, x, mask, knn_memory = None):\n",
        "    if knn_memory is None:\n",
        "      x = self.attention(x, mask)\n",
        "    else:\n",
        "      x = self.attention(x, mask, knn_memory)\n",
        "    x = self.dropout1(x + self.norm1(x))\n",
        "    x = x + self.ff(x)\n",
        "    x = self.dropout2(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "prYm-KEpwLQq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module): #block of the classic transformer decoder (not used)\n",
        "  def __init__(self, d, h, batch_size, hidden_size, dropout):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(d, h, batch_size)\n",
        "    self.norm = nn.LayerNorm(d)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.transformer_block = TransformerBlock(d, h, batch_size, hidden_size, dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.attention(x, mask)\n",
        "    x = self.dropout(self.norm(x))\n",
        "    return self.transformer_block(x)"
      ],
      "metadata": {
        "id": "P4fkZ_djysyF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemorizingTransformer(nn.Module):\n",
        "    def __init__(\n",
        "          self,\n",
        "          num_tokens,\n",
        "          d,\n",
        "          heads = 8,\n",
        "          depth = 4,\n",
        "          knn_attn_idx = 2,\n",
        "          attn_dropout = 0.,\n",
        "          hidden_size = 1000,\n",
        "          dropout = 0.3,\n",
        "          max_knn_memories = 1000,\n",
        "          num_retrieved_memories = 32,\n",
        "          batch_size = 16,\n",
        "          use_bert = True\n",
        "      ):\n",
        "          # asserts\n",
        "          self.d = d if not use_bert else 768\n",
        "          assert self.d % heads == 0\n",
        "          assert knn_attn_idx < depth\n",
        "\n",
        "          super(MemorizingTransformer, self).__init__()\n",
        "          #self.token_emb = nn.Embedding(num_tokens, self.d) #without BERT\n",
        "          self.token_emb = BertModel.from_pretrained('bert-base-uncased')\n",
        "          self.positional_enc = PositionalEncoding(self.d, max_len = 5000)\n",
        "          self.dim_head = self.d // heads\n",
        "          \n",
        "          self.heads = heads\n",
        "          self.knn_attn_idx = knn_attn_idx\n",
        "          self.depth = depth\n",
        "          self.attn_dropout = attn_dropout\n",
        "          self.hidden_size = hidden_size\n",
        "          self.dropout = dropout\n",
        "          self.max_knn_memories = max_knn_memories\n",
        "          self.num_retrieved_memories = num_retrieved_memories\n",
        "          self.batch_size = batch_size\n",
        "\n",
        "          self.layers = nn.ModuleList([])\n",
        "          for idx in range(depth):\n",
        "            self.layers.append(\n",
        "                TransformerBlock(self.d, heads, batch_size, hidden_size, dropout, is_mem = idx == self.knn_attn_idx)\n",
        "            )\n",
        "\n",
        "          self.to_out = nn.Linear(self.d, num_tokens)\n",
        "    \n",
        "    def create_mask(self, x):\n",
        "      batch_size, seq_len = x.shape\n",
        "      mask = torch.tril(torch.ones((seq_len, seq_len))).expand(\n",
        "          batch_size, 1, seq_len, seq_len)\n",
        "      return mask    \n",
        "          \n",
        "    def forward(self, x, knn_memory):\n",
        "      mask = self.create_mask(x)\n",
        "      #x = self.token_emb(x) #without BERT\n",
        "      x = self.token_emb(x)[0] #with BERT\n",
        "      x = self.positional_enc(x)\n",
        "\n",
        "      for idx in range(self.depth):\n",
        "          x= self.layers[idx](x, mask, knn_memory = knn_memory if idx == self.knn_attn_idx else None)\n",
        "\n",
        "      return self.to_out(x).transpose(1, 2)"
      ],
      "metadata": {
        "id": "F5uqCzrVL569"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ihBnTrPhaiDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "SEQ_LEN = 256\n",
        "HEADS = 8\n",
        "DIM_HEAD = SEQ_LEN // HEADS \n",
        "DIM_HEAD_BERT = 768 // HEADS #it's 96 with bert (768/8)\n",
        "\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_GRAD_CLIP_NORM = 0.5\n",
        "\n",
        "EVAL_EVERY = 1\n",
        "CHECKPOINT = 1"
      ],
      "metadata": {
        "id": "ncWOJOkFanTK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "memory = None\n",
        "data = None\n",
        "train_loader_ = None\n",
        "test_loader_ = None"
      ],
      "metadata": {
        "id": "kWjcjNDgEiuS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MemorizingTransformer(\n",
        "    num_tokens = vocabulary,\n",
        "    d = SEQ_LEN,\n",
        "    heads = HEADS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    depth = 4,\n",
        "    knn_attn_idx = 2,\n",
        "    num_retrieved_memories = 32,\n",
        "    use_bert = True #False if you want to try without BERT\n",
        ").to(device)\n",
        "\n",
        "memory = KNNMemory(\n",
        "    dim = DIM_HEAD_BERT,       #substitute with DIM_HEAD if you want to try without BERT\n",
        "    max_memories = 1000,       #maximum number of memories (old ones will be discarded after reaching maximum capacity)\n",
        "    num_indices = BATCH_SIZE   #each batch keeps track of its own memories, expiring when it sees a new document\n",
        ")\n",
        "\n",
        "train_loader_ = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)\n",
        "test_loader_ = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)"
      ],
      "metadata": {
        "id": "1gVvmKCm3J4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d44067-8d49-4c5c-d661-24ee9874c809"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 160506.73B/s]\n",
            "100%|██████████| 440473133/440473133 [00:08<00:00, 51585143.55B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "# training\n",
        "\n",
        "perplexity_list = []\n",
        "\n",
        "for e in range(epochs):\n",
        "  for i, data in enumerate(tqdm.tqdm(train_loader_, desc = 'training')):\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0.\n",
        "\n",
        "    num_seq = 10000 // (SEQ_LEN + 1)\n",
        "    data = data.long().to(device)\n",
        "    for j in range(num_seq):\n",
        "      mini_batch = data[:, j*(SEQ_LEN + 1):(j+1)*(SEQ_LEN + 1)]\n",
        "      seq, labels = mini_batch[:, :-1], mini_batch[:, 1:]\n",
        "      out = model(\n",
        "        seq,\n",
        "        knn_memory = memory\n",
        "      )\n",
        "      loss_item = loss(out, labels)\n",
        "      print(f'training loss: {loss_item}', flush = True)\n",
        "      loss_item.backward() \n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "  data = None\n",
        "\n",
        "  if e % EVAL_EVERY == 0:\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      metric = Perplexity().to(device)\n",
        "      for i, data in enumerate(tqdm.tqdm(test_loader_, desc = 'evaluation')):\n",
        "        num_seq = 10000 // (SEQ_LEN + 1)\n",
        "        data = data.long().to(device)\n",
        "\n",
        "        for j in range(num_seq):\n",
        "          mini_batch = data[:, j*(SEQ_LEN + 1):(j+1)*(SEQ_LEN + 1)]\n",
        "          seq, labels = mini_batch[:, :-1], mini_batch[:, 1:]\n",
        "          out = model(\n",
        "            seq,\n",
        "            knn_memory = memory\n",
        "          )\n",
        "          test_loss = loss(out, labels)\n",
        "          metric(out.transpose(1, 2), labels)\n",
        "          #print(f'test loss: {test_loss}', flush = True)\n",
        "\n",
        "      perplexity = metric.compute()\n",
        "      perplexity_list.append(perplexity.to(\"cpu\").item())\n",
        "      print(f'perplexity: {perplexity}', flush = True)\n",
        "\n",
        "  data = None\n",
        "  if e % CHECKPOINT == 0:\n",
        "    torch.save({\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, 'model_optimizer.pt')\n",
        "    \"\"\"\n",
        "    #Lorenzo\n",
        "    with open('/content/drive/MyDrive/Università/Magistrale/Secondo Anno/Neural Networks/project/perplexity_moreNN.npy', 'wb') as f:\n",
        "      np.save(f, np.array(perplexity_list))\n",
        "    \"\"\"\n",
        "    #Luigi\n",
        "    with open(f'drive/MyDrive/Colab Notebooks/perplexity_memorizing_tr.pkl', 'wb') as pklfile:\n",
        "      pkl.dump(perplexity_list, pklfile)\n"
      ],
      "metadata": {
        "id": "9sF-vlC6koIo",
        "outputId": "151d7605-c8bc-4cc0-c5a9-7db0f144d88d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   0%|          | 0/60 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 10.54646110534668\n",
            "training loss: 9.88571548461914\n",
            "training loss: 9.494729995727539\n",
            "training loss: 9.24975299835205\n",
            "training loss: 8.952232360839844\n",
            "training loss: 8.984757423400879\n",
            "training loss: 8.778602600097656\n",
            "training loss: 8.824331283569336\n",
            "training loss: 8.439093589782715\n",
            "training loss: 8.2456636428833\n",
            "training loss: 8.247018814086914\n",
            "training loss: 8.211359024047852\n",
            "training loss: 8.04597282409668\n",
            "training loss: 7.948552131652832\n",
            "training loss: 7.961156368255615\n",
            "training loss: 7.642862319946289\n",
            "training loss: 7.630606651306152\n",
            "training loss: 7.545746803283691\n",
            "training loss: 7.423794746398926\n",
            "training loss: 7.525940418243408\n",
            "training loss: 7.629098892211914\n",
            "training loss: 7.306756973266602\n",
            "training loss: 7.2343950271606445\n",
            "training loss: 7.273968696594238\n",
            "training loss: 7.01867151260376\n",
            "training loss: 7.075180530548096\n",
            "training loss: 7.211997985839844\n",
            "training loss: 6.955669403076172\n",
            "training loss: 6.867842674255371\n",
            "training loss: 6.932401657104492\n",
            "training loss: 6.759255409240723\n",
            "training loss: 6.932572364807129\n",
            "training loss: 6.816497325897217\n",
            "training loss: 6.83335018157959\n",
            "training loss: 6.625648021697998\n",
            "training loss: 6.898165702819824\n",
            "training loss: 6.7017741203308105\n",
            "training loss: 6.481013774871826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   2%|▏         | 1/60 [00:45<44:53, 45.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.07674503326416\n",
            "training loss: 7.350062370300293\n",
            "training loss: 7.682126045227051\n",
            "training loss: 7.325775623321533\n",
            "training loss: 7.296627998352051\n",
            "training loss: 7.159310340881348\n",
            "training loss: 7.348682880401611\n",
            "training loss: 6.959245681762695\n",
            "training loss: 7.354015827178955\n",
            "training loss: 7.1642584800720215\n",
            "training loss: 7.172267913818359\n",
            "training loss: 6.827927589416504\n",
            "training loss: 7.146836280822754\n",
            "training loss: 7.308200359344482\n",
            "training loss: 7.187881946563721\n",
            "training loss: 6.957906723022461\n",
            "training loss: 6.981860160827637\n",
            "training loss: 6.947226524353027\n",
            "training loss: 6.931074619293213\n",
            "training loss: 7.094704627990723\n",
            "training loss: 7.023993492126465\n",
            "training loss: 6.994158744812012\n",
            "training loss: 6.947716236114502\n",
            "training loss: 7.383116245269775\n",
            "training loss: 7.140150547027588\n",
            "training loss: 7.106842041015625\n",
            "training loss: 7.054574012756348\n",
            "training loss: 7.202321529388428\n",
            "training loss: 6.994464874267578\n",
            "training loss: 7.01807975769043\n",
            "training loss: 6.662868499755859\n",
            "training loss: 6.926966667175293\n",
            "training loss: 6.918532371520996\n",
            "training loss: 7.017111778259277\n",
            "training loss: 7.1073408126831055\n",
            "training loss: 6.709780216217041\n",
            "training loss: 6.885407447814941\n",
            "training loss: 7.177826404571533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   3%|▎         | 2/60 [01:21<38:27, 39.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.1085944175720215\n",
            "training loss: 7.0927734375\n",
            "training loss: 7.224176406860352\n",
            "training loss: 7.234002113342285\n",
            "training loss: 7.192707061767578\n",
            "training loss: 7.039185523986816\n",
            "training loss: 7.2702226638793945\n",
            "training loss: 7.3756632804870605\n",
            "training loss: 7.222468852996826\n",
            "training loss: 6.903106689453125\n",
            "training loss: 6.995551586151123\n",
            "training loss: 7.226097583770752\n",
            "training loss: 7.306081295013428\n",
            "training loss: 7.228246212005615\n",
            "training loss: 6.9522294998168945\n",
            "training loss: 6.848288536071777\n",
            "training loss: 7.105694770812988\n",
            "training loss: 7.0927629470825195\n",
            "training loss: 7.034696578979492\n",
            "training loss: 6.880264759063721\n",
            "training loss: 7.117188930511475\n",
            "training loss: 6.875316143035889\n",
            "training loss: 7.06588077545166\n",
            "training loss: 6.852053642272949\n",
            "training loss: 6.91294002532959\n",
            "training loss: 6.7944111824035645\n",
            "training loss: 6.971394062042236\n",
            "training loss: 6.739168167114258\n",
            "training loss: 6.661280632019043\n",
            "training loss: 6.8002777099609375\n",
            "training loss: 6.942897319793701\n",
            "training loss: 7.006543159484863\n",
            "training loss: 7.125413417816162\n",
            "training loss: 6.845880508422852\n",
            "training loss: 6.9436421394348145\n",
            "training loss: 6.850368022918701\n",
            "training loss: 6.988918304443359\n",
            "training loss: 6.894016265869141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   5%|▌         | 3/60 [01:56<35:54, 37.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.753293037414551\n",
            "training loss: 7.068434715270996\n",
            "training loss: 7.261845111846924\n",
            "training loss: 7.171039581298828\n",
            "training loss: 7.325767517089844\n",
            "training loss: 7.098796367645264\n",
            "training loss: 7.281352996826172\n",
            "training loss: 7.054267406463623\n",
            "training loss: 6.892285346984863\n",
            "training loss: 6.938602447509766\n",
            "training loss: 6.856549263000488\n",
            "training loss: 6.957758903503418\n",
            "training loss: 7.217504501342773\n",
            "training loss: 6.836620807647705\n",
            "training loss: 6.73714542388916\n",
            "training loss: 6.821091651916504\n",
            "training loss: 6.8143463134765625\n",
            "training loss: 7.082614421844482\n",
            "training loss: 6.860550880432129\n",
            "training loss: 6.908273696899414\n",
            "training loss: 6.843075275421143\n",
            "training loss: 6.9333577156066895\n",
            "training loss: 7.028016090393066\n",
            "training loss: 6.567833423614502\n",
            "training loss: 7.085200309753418\n",
            "training loss: 6.997123718261719\n",
            "training loss: 7.067849159240723\n",
            "training loss: 6.874339580535889\n",
            "training loss: 6.847751617431641\n",
            "training loss: 7.056138038635254\n",
            "training loss: 6.787135124206543\n",
            "training loss: 6.917925834655762\n",
            "training loss: 6.9132490158081055\n",
            "training loss: 6.918229103088379\n",
            "training loss: 7.022653579711914\n",
            "training loss: 6.8751420974731445\n",
            "training loss: 6.777861595153809\n",
            "training loss: 6.793374061584473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   7%|▋         | 4/60 [02:33<34:53, 37.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.272364139556885\n",
            "training loss: 6.740130424499512\n",
            "training loss: 6.8972649574279785\n",
            "training loss: 7.030448913574219\n",
            "training loss: 6.704068183898926\n",
            "training loss: 6.966228485107422\n",
            "training loss: 7.104859352111816\n",
            "training loss: 6.841655731201172\n",
            "training loss: 6.633854866027832\n",
            "training loss: 7.3416748046875\n",
            "training loss: 7.252379417419434\n",
            "training loss: 7.402961730957031\n",
            "training loss: 7.210440635681152\n",
            "training loss: 7.098328590393066\n",
            "training loss: 7.064961910247803\n",
            "training loss: 7.103145599365234\n",
            "training loss: 7.155771732330322\n",
            "training loss: 7.09312629699707\n",
            "training loss: 7.284006595611572\n",
            "training loss: 7.116166114807129\n",
            "training loss: 7.36961555480957\n",
            "training loss: 7.088294982910156\n",
            "training loss: 7.231110095977783\n",
            "training loss: 6.891471862792969\n",
            "training loss: 6.924946308135986\n",
            "training loss: 6.607790946960449\n",
            "training loss: 6.960866928100586\n",
            "training loss: 7.078051567077637\n",
            "training loss: 7.28059720993042\n",
            "training loss: 7.001684188842773\n",
            "training loss: 7.119235992431641\n",
            "training loss: 7.263354301452637\n",
            "training loss: 6.981868743896484\n",
            "training loss: 6.765854835510254\n",
            "training loss: 6.9051289558410645\n",
            "training loss: 6.711028099060059\n",
            "training loss: 6.426818370819092\n",
            "training loss: 6.896089553833008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   8%|▊         | 5/60 [03:09<33:50, 36.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.922350883483887\n",
            "training loss: 6.9477033615112305\n",
            "training loss: 7.459865570068359\n",
            "training loss: 7.307991027832031\n",
            "training loss: 7.127310276031494\n",
            "training loss: 6.704906940460205\n",
            "training loss: 6.8943939208984375\n",
            "training loss: 6.854826927185059\n",
            "training loss: 7.185061931610107\n",
            "training loss: 7.019917011260986\n",
            "training loss: 7.018768310546875\n",
            "training loss: 7.028637409210205\n",
            "training loss: 6.769926071166992\n",
            "training loss: 6.8797607421875\n",
            "training loss: 6.954030990600586\n",
            "training loss: 6.900911808013916\n",
            "training loss: 6.757037162780762\n",
            "training loss: 6.57227897644043\n",
            "training loss: 6.484607696533203\n",
            "training loss: 6.605151176452637\n",
            "training loss: 6.803582191467285\n",
            "training loss: 6.93224573135376\n",
            "training loss: 6.845978736877441\n",
            "training loss: 6.905041217803955\n",
            "training loss: 6.955655097961426\n",
            "training loss: 6.741855621337891\n",
            "training loss: 6.79434061050415\n",
            "training loss: 6.7589192390441895\n",
            "training loss: 6.79114294052124\n",
            "training loss: 6.923149108886719\n",
            "training loss: 7.111248016357422\n",
            "training loss: 7.036041259765625\n",
            "training loss: 6.70021915435791\n",
            "training loss: 6.800393581390381\n",
            "training loss: 6.837082862854004\n",
            "training loss: 6.749029636383057\n",
            "training loss: 6.8162522315979\n",
            "training loss: 6.687525749206543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  10%|█         | 6/60 [03:46<33:04, 36.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.274611473083496\n",
            "training loss: 7.330661296844482\n",
            "training loss: 7.255833625793457\n",
            "training loss: 7.217911720275879\n",
            "training loss: 6.974493026733398\n",
            "training loss: 7.323848724365234\n",
            "training loss: 7.240365982055664\n",
            "training loss: 7.12738037109375\n",
            "training loss: 7.035337448120117\n",
            "training loss: 6.944636821746826\n",
            "training loss: 7.017834186553955\n",
            "training loss: 6.751279830932617\n",
            "training loss: 6.874782562255859\n",
            "training loss: 7.166088581085205\n",
            "training loss: 7.25240421295166\n",
            "training loss: 7.019883155822754\n",
            "training loss: 6.745367050170898\n",
            "training loss: 7.069219589233398\n",
            "training loss: 7.183108329772949\n",
            "training loss: 7.0988311767578125\n",
            "training loss: 6.8211798667907715\n",
            "training loss: 6.884946346282959\n",
            "training loss: 7.196321964263916\n",
            "training loss: 6.948347091674805\n",
            "training loss: 6.843062400817871\n",
            "training loss: 6.9426679611206055\n",
            "training loss: 6.998650074005127\n",
            "training loss: 6.698050498962402\n",
            "training loss: 7.113221645355225\n",
            "training loss: 6.855042457580566\n",
            "training loss: 6.917169570922852\n",
            "training loss: 7.010164737701416\n",
            "training loss: 7.2323408126831055\n",
            "training loss: 6.958681106567383\n",
            "training loss: 6.788450241088867\n",
            "training loss: 7.030744552612305\n",
            "training loss: 6.741628646850586\n",
            "training loss: 6.668306827545166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  12%|█▏        | 7/60 [04:21<32:04, 36.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.12842321395874\n",
            "training loss: 7.032323837280273\n",
            "training loss: 7.146127700805664\n",
            "training loss: 7.111433029174805\n",
            "training loss: 7.165432929992676\n",
            "training loss: 7.248120307922363\n",
            "training loss: 7.041353225708008\n",
            "training loss: 6.8366618156433105\n",
            "training loss: 6.946352958679199\n",
            "training loss: 6.706109046936035\n",
            "training loss: 6.664897918701172\n",
            "training loss: 6.715521812438965\n",
            "training loss: 6.914601802825928\n",
            "training loss: 6.641655445098877\n",
            "training loss: 6.710521697998047\n",
            "training loss: 6.6629638671875\n",
            "training loss: 6.65829610824585\n",
            "training loss: 6.761299133300781\n",
            "training loss: 6.395857810974121\n",
            "training loss: 6.753324508666992\n",
            "training loss: 6.707558631896973\n",
            "training loss: 6.848833084106445\n",
            "training loss: 6.8091535568237305\n",
            "training loss: 6.795732021331787\n",
            "training loss: 6.655832290649414\n",
            "training loss: 6.9913482666015625\n",
            "training loss: 6.590849876403809\n",
            "training loss: 6.989608287811279\n",
            "training loss: 6.812255859375\n",
            "training loss: 6.9377360343933105\n",
            "training loss: 6.638932228088379\n",
            "training loss: 6.966732025146484\n",
            "training loss: 7.090420722961426\n",
            "training loss: 6.777731418609619\n",
            "training loss: 6.83561897277832\n",
            "training loss: 6.734988689422607\n",
            "training loss: 6.5949225425720215\n",
            "training loss: 6.787412643432617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  13%|█▎        | 8/60 [04:56<31:13, 36.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.203726768493652\n",
            "training loss: 7.399287223815918\n",
            "training loss: 7.1083173751831055\n",
            "training loss: 7.116846561431885\n",
            "training loss: 6.914628982543945\n",
            "training loss: 6.969188690185547\n",
            "training loss: 6.785020351409912\n",
            "training loss: 7.013864040374756\n",
            "training loss: 7.2166948318481445\n",
            "training loss: 7.156347274780273\n",
            "training loss: 7.177513122558594\n",
            "training loss: 7.014917373657227\n",
            "training loss: 6.830170631408691\n",
            "training loss: 6.987125873565674\n",
            "training loss: 7.021543502807617\n",
            "training loss: 7.216692924499512\n",
            "training loss: 6.887724876403809\n",
            "training loss: 6.711747169494629\n",
            "training loss: 6.93192195892334\n",
            "training loss: 6.900523662567139\n",
            "training loss: 7.020869255065918\n",
            "training loss: 6.751094818115234\n",
            "training loss: 6.890288352966309\n",
            "training loss: 6.893332481384277\n",
            "training loss: 7.024255275726318\n",
            "training loss: 6.937777519226074\n",
            "training loss: 6.879469394683838\n",
            "training loss: 7.159651756286621\n",
            "training loss: 7.137111663818359\n",
            "training loss: 6.894409656524658\n",
            "training loss: 6.93651819229126\n",
            "training loss: 6.987784385681152\n",
            "training loss: 6.939821243286133\n",
            "training loss: 6.835444450378418\n",
            "training loss: 6.848501205444336\n",
            "training loss: 7.062079429626465\n",
            "training loss: 6.929228782653809\n",
            "training loss: 7.063427925109863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  15%|█▌        | 9/60 [05:37<31:46, 37.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.116459846496582\n",
            "training loss: 7.288442134857178\n",
            "training loss: 6.888761520385742\n",
            "training loss: 6.6833295822143555\n",
            "training loss: 6.763093948364258\n",
            "training loss: 7.064169883728027\n",
            "training loss: 6.9923481941223145\n",
            "training loss: 6.957213401794434\n",
            "training loss: 6.897931098937988\n",
            "training loss: 6.656078338623047\n",
            "training loss: 6.888482570648193\n",
            "training loss: 6.881093978881836\n",
            "training loss: 6.5283708572387695\n",
            "training loss: 6.417605400085449\n",
            "training loss: 6.192713260650635\n",
            "training loss: 6.399835109710693\n",
            "training loss: 6.719776153564453\n",
            "training loss: 6.666652679443359\n",
            "training loss: 6.970160007476807\n",
            "training loss: 7.144123077392578\n",
            "training loss: 6.995254993438721\n",
            "training loss: 7.045685768127441\n",
            "training loss: 7.271467208862305\n",
            "training loss: 6.973357200622559\n",
            "training loss: 7.039099216461182\n",
            "training loss: 6.964046955108643\n",
            "training loss: 7.266706466674805\n",
            "training loss: 7.242124080657959\n",
            "training loss: 7.225002288818359\n",
            "training loss: 6.900221824645996\n",
            "training loss: 6.7920427322387695\n",
            "training loss: 7.013208389282227\n",
            "training loss: 6.771661281585693\n",
            "training loss: 6.947543144226074\n",
            "training loss: 6.857390403747559\n",
            "training loss: 6.876234531402588\n",
            "training loss: 7.006961822509766\n",
            "training loss: 6.792784690856934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  17%|█▋        | 10/60 [06:14<31:04, 37.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.067478179931641\n",
            "training loss: 7.029695510864258\n",
            "training loss: 7.146841049194336\n",
            "training loss: 7.067526817321777\n",
            "training loss: 7.252726078033447\n",
            "training loss: 6.9554643630981445\n",
            "training loss: 6.979473114013672\n",
            "training loss: 7.233005046844482\n",
            "training loss: 6.8090033531188965\n",
            "training loss: 7.21370792388916\n",
            "training loss: 6.935129165649414\n",
            "training loss: 7.278772354125977\n",
            "training loss: 7.020476818084717\n",
            "training loss: 6.8721513748168945\n",
            "training loss: 7.100039482116699\n",
            "training loss: 6.9212188720703125\n",
            "training loss: 6.842740058898926\n",
            "training loss: 6.758708953857422\n",
            "training loss: 6.738121032714844\n",
            "training loss: 6.713260650634766\n",
            "training loss: 6.90215539932251\n",
            "training loss: 6.988504409790039\n",
            "training loss: 7.017724514007568\n",
            "training loss: 7.025184154510498\n",
            "training loss: 7.070982933044434\n",
            "training loss: 7.010251045227051\n",
            "training loss: 6.846658706665039\n",
            "training loss: 6.947325706481934\n",
            "training loss: 6.960505485534668\n",
            "training loss: 6.951962947845459\n",
            "training loss: 6.909172058105469\n",
            "training loss: 6.74535608291626\n",
            "training loss: 6.901718616485596\n",
            "training loss: 6.943915367126465\n",
            "training loss: 6.717667102813721\n",
            "training loss: 6.634000778198242\n",
            "training loss: 6.954181671142578\n",
            "training loss: 6.803166389465332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  18%|█▊        | 11/60 [06:49<29:57, 36.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.9811577796936035\n",
            "training loss: 7.200235366821289\n",
            "training loss: 7.105599403381348\n",
            "training loss: 6.689648628234863\n",
            "training loss: 7.023853778839111\n",
            "training loss: 7.013182163238525\n",
            "training loss: 6.877403259277344\n",
            "training loss: 6.932652473449707\n",
            "training loss: 6.843267440795898\n",
            "training loss: 6.927739143371582\n",
            "training loss: 7.086334228515625\n",
            "training loss: 6.869014739990234\n",
            "training loss: 6.852719783782959\n",
            "training loss: 6.751747131347656\n",
            "training loss: 6.826826572418213\n",
            "training loss: 6.846612930297852\n",
            "training loss: 6.6284284591674805\n",
            "training loss: 6.820945739746094\n",
            "training loss: 6.509188652038574\n",
            "training loss: 6.7959489822387695\n",
            "training loss: 6.881278991699219\n",
            "training loss: 6.946660041809082\n",
            "training loss: 6.909499645233154\n",
            "training loss: 6.992371559143066\n",
            "training loss: 7.025063514709473\n",
            "training loss: 6.858589172363281\n",
            "training loss: 6.876674652099609\n",
            "training loss: 6.84967041015625\n",
            "training loss: 7.044525623321533\n",
            "training loss: 6.7703351974487305\n",
            "training loss: 6.768482685089111\n",
            "training loss: 6.983902454376221\n",
            "training loss: 7.089305877685547\n",
            "training loss: 6.768041610717773\n",
            "training loss: 7.186433792114258\n",
            "training loss: 6.860105037689209\n",
            "training loss: 6.681779861450195\n",
            "training loss: 6.57252311706543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  20%|██        | 12/60 [07:25<29:03, 36.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.365132808685303\n",
            "training loss: 6.862766265869141\n",
            "training loss: 6.691547393798828\n",
            "training loss: 7.016526222229004\n",
            "training loss: 6.98640251159668\n",
            "training loss: 6.397640228271484\n",
            "training loss: 6.752723217010498\n",
            "training loss: 6.939139366149902\n",
            "training loss: 7.133696556091309\n",
            "training loss: 6.94575309753418\n",
            "training loss: 6.964570045471191\n",
            "training loss: 7.091745376586914\n",
            "training loss: 6.969285488128662\n",
            "training loss: 6.910910129547119\n",
            "training loss: 6.903585910797119\n",
            "training loss: 6.894765853881836\n",
            "training loss: 6.604221343994141\n",
            "training loss: 6.87443733215332\n",
            "training loss: 6.809235095977783\n",
            "training loss: 6.823818683624268\n",
            "training loss: 7.040209770202637\n",
            "training loss: 7.052953243255615\n",
            "training loss: 6.895357131958008\n",
            "training loss: 6.786706447601318\n",
            "training loss: 7.063474178314209\n",
            "training loss: 6.869497776031494\n",
            "training loss: 6.745908260345459\n",
            "training loss: 6.743241310119629\n",
            "training loss: 6.658281326293945\n",
            "training loss: 6.930745601654053\n",
            "training loss: 6.752234935760498\n",
            "training loss: 6.849156379699707\n",
            "training loss: 6.848052024841309\n",
            "training loss: 6.669714450836182\n",
            "training loss: 6.877444744110107\n",
            "training loss: 6.714444637298584\n",
            "training loss: 6.750616550445557\n",
            "training loss: 6.648460388183594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  22%|██▏       | 13/60 [08:00<28:11, 35.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.141006946563721\n",
            "training loss: 7.108498573303223\n",
            "training loss: 7.047390937805176\n",
            "training loss: 6.98895263671875\n",
            "training loss: 6.981597900390625\n",
            "training loss: 7.086544990539551\n",
            "training loss: 6.975973606109619\n",
            "training loss: 6.72996711730957\n",
            "training loss: 6.630702972412109\n",
            "training loss: 6.8985700607299805\n",
            "training loss: 6.86870002746582\n",
            "training loss: 6.925694465637207\n",
            "training loss: 6.824066638946533\n",
            "training loss: 7.153515815734863\n",
            "training loss: 6.8810200691223145\n",
            "training loss: 6.866385459899902\n",
            "training loss: 6.8437957763671875\n",
            "training loss: 7.066636562347412\n",
            "training loss: 7.184277057647705\n",
            "training loss: 6.653055191040039\n",
            "training loss: 6.838535785675049\n",
            "training loss: 6.951874256134033\n",
            "training loss: 6.970311164855957\n",
            "training loss: 6.963677883148193\n",
            "training loss: 6.8155999183654785\n",
            "training loss: 6.7799153327941895\n",
            "training loss: 6.994291305541992\n",
            "training loss: 6.725350856781006\n",
            "training loss: 6.857416152954102\n",
            "training loss: 6.736697673797607\n",
            "training loss: 7.04705810546875\n",
            "training loss: 7.031207084655762\n",
            "training loss: 6.605749130249023\n",
            "training loss: 6.747533798217773\n",
            "training loss: 6.837006092071533\n",
            "training loss: 6.771666049957275\n",
            "training loss: 7.057599067687988\n",
            "training loss: 7.23283052444458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  23%|██▎       | 14/60 [08:35<27:24, 35.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.281933784484863\n",
            "training loss: 7.191170692443848\n",
            "training loss: 7.178450584411621\n",
            "training loss: 7.302396774291992\n",
            "training loss: 7.272289276123047\n",
            "training loss: 7.085412502288818\n",
            "training loss: 7.266282081604004\n",
            "training loss: 6.989983081817627\n",
            "training loss: 7.0414628982543945\n",
            "training loss: 7.113045692443848\n",
            "training loss: 7.0110368728637695\n",
            "training loss: 6.985955238342285\n",
            "training loss: 6.954500198364258\n",
            "training loss: 6.676636695861816\n",
            "training loss: 6.73261833190918\n",
            "training loss: 6.765750408172607\n",
            "training loss: 6.883269309997559\n",
            "training loss: 7.060221195220947\n",
            "training loss: 7.100110054016113\n",
            "training loss: 6.892762184143066\n",
            "training loss: 7.1408538818359375\n",
            "training loss: 7.012520790100098\n",
            "training loss: 6.870570182800293\n",
            "training loss: 6.894813537597656\n",
            "training loss: 6.903322219848633\n",
            "training loss: 6.924929141998291\n",
            "training loss: 6.916597366333008\n",
            "training loss: 6.987645626068115\n",
            "training loss: 6.969101905822754\n",
            "training loss: 6.71989631652832\n",
            "training loss: 6.870406627655029\n",
            "training loss: 6.812493324279785\n",
            "training loss: 6.846469879150391\n",
            "training loss: 6.935636043548584\n",
            "training loss: 6.94327449798584\n",
            "training loss: 6.899233818054199\n",
            "training loss: 6.752208709716797\n",
            "training loss: 6.711821556091309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  25%|██▌       | 15/60 [09:10<26:41, 35.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.9938178062438965\n",
            "training loss: 6.7686357498168945\n",
            "training loss: 7.1225199699401855\n",
            "training loss: 7.226471900939941\n",
            "training loss: 7.166265964508057\n",
            "training loss: 7.277108669281006\n",
            "training loss: 7.061692237854004\n",
            "training loss: 7.256224155426025\n",
            "training loss: 7.265787601470947\n",
            "training loss: 7.258070468902588\n",
            "training loss: 7.278071880340576\n",
            "training loss: 7.054356098175049\n",
            "training loss: 7.171854019165039\n",
            "training loss: 6.76068115234375\n",
            "training loss: 7.008023262023926\n",
            "training loss: 6.991521835327148\n",
            "training loss: 6.718414306640625\n",
            "training loss: 6.937644958496094\n",
            "training loss: 7.1078033447265625\n",
            "training loss: 6.815119743347168\n",
            "training loss: 6.899244785308838\n",
            "training loss: 6.925075054168701\n",
            "training loss: 7.166835784912109\n",
            "training loss: 7.106078147888184\n",
            "training loss: 7.086829662322998\n",
            "training loss: 6.539505958557129\n",
            "training loss: 6.817439079284668\n",
            "training loss: 7.140496253967285\n",
            "training loss: 7.112853050231934\n",
            "training loss: 6.607953071594238\n",
            "training loss: 6.871203422546387\n",
            "training loss: 6.960287094116211\n",
            "training loss: 6.950357913970947\n",
            "training loss: 6.777284145355225\n",
            "training loss: 6.772489070892334\n",
            "training loss: 6.5676984786987305\n",
            "training loss: 6.665309906005859\n",
            "training loss: 6.990562438964844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  27%|██▋       | 16/60 [09:45<26:01, 35.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.000051498413086\n",
            "training loss: 7.0959062576293945\n",
            "training loss: 7.303802013397217\n",
            "training loss: 7.096024513244629\n",
            "training loss: 7.260683059692383\n",
            "training loss: 7.097616195678711\n",
            "training loss: 7.0063276290893555\n",
            "training loss: 6.914802074432373\n",
            "training loss: 7.007904529571533\n",
            "training loss: 6.883886337280273\n",
            "training loss: 6.94601583480835\n",
            "training loss: 7.0642991065979\n",
            "training loss: 6.939962387084961\n",
            "training loss: 7.135066986083984\n",
            "training loss: 6.965516090393066\n",
            "training loss: 7.016519069671631\n",
            "training loss: 7.030120849609375\n",
            "training loss: 7.033968448638916\n",
            "training loss: 7.011713981628418\n",
            "training loss: 6.860851764678955\n",
            "training loss: 6.841172218322754\n",
            "training loss: 7.056299686431885\n",
            "training loss: 7.055541038513184\n",
            "training loss: 7.215261936187744\n",
            "training loss: 7.179925918579102\n",
            "training loss: 7.163534164428711\n",
            "training loss: 6.937739372253418\n",
            "training loss: 6.850292205810547\n",
            "training loss: 6.875326156616211\n",
            "training loss: 6.79871129989624\n",
            "training loss: 6.532064914703369\n",
            "training loss: 6.853584289550781\n",
            "training loss: 6.905981063842773\n",
            "training loss: 6.907000541687012\n",
            "training loss: 6.996037483215332\n",
            "training loss: 6.779206275939941\n",
            "training loss: 6.940878391265869\n",
            "training loss: 6.944667816162109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  28%|██▊       | 17/60 [10:22<25:40, 35.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.107355117797852\n",
            "training loss: 7.2675065994262695\n",
            "training loss: 7.020158290863037\n",
            "training loss: 7.037923812866211\n",
            "training loss: 6.892134189605713\n",
            "training loss: 6.806954383850098\n",
            "training loss: 6.99786376953125\n",
            "training loss: 6.807233810424805\n",
            "training loss: 6.790877342224121\n",
            "training loss: 6.806056499481201\n",
            "training loss: 6.726467132568359\n",
            "training loss: 6.888118743896484\n",
            "training loss: 6.850795269012451\n",
            "training loss: 6.88065767288208\n",
            "training loss: 7.131885528564453\n",
            "training loss: 7.001470565795898\n",
            "training loss: 7.043973922729492\n",
            "training loss: 6.822731971740723\n",
            "training loss: 6.977272033691406\n",
            "training loss: 6.98406982421875\n",
            "training loss: 6.968888282775879\n",
            "training loss: 6.684197902679443\n",
            "training loss: 6.9174041748046875\n",
            "training loss: 7.097179412841797\n",
            "training loss: 6.89752197265625\n",
            "training loss: 6.889096260070801\n",
            "training loss: 7.023492336273193\n",
            "training loss: 6.998906135559082\n",
            "training loss: 6.766718864440918\n",
            "training loss: 6.53568172454834\n",
            "training loss: 6.909531593322754\n",
            "training loss: 6.900012016296387\n",
            "training loss: 6.964404582977295\n",
            "training loss: 6.738167762756348\n",
            "training loss: 6.764060020446777\n",
            "training loss: 6.7561798095703125\n",
            "training loss: 6.8372697830200195\n",
            "training loss: 6.676486015319824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  30%|███       | 18/60 [10:57<24:54, 35.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.263477802276611\n",
            "training loss: 7.115840911865234\n",
            "training loss: 7.2140092849731445\n",
            "training loss: 6.866522312164307\n",
            "training loss: 7.19847297668457\n",
            "training loss: 7.258071422576904\n",
            "training loss: 7.152855396270752\n",
            "training loss: 7.286255359649658\n",
            "training loss: 6.771519184112549\n",
            "training loss: 6.964633464813232\n",
            "training loss: 7.123668193817139\n",
            "training loss: 6.943536758422852\n",
            "training loss: 7.0328216552734375\n",
            "training loss: 6.943026542663574\n",
            "training loss: 7.040861129760742\n",
            "training loss: 7.011700630187988\n",
            "training loss: 7.008667945861816\n",
            "training loss: 6.777279376983643\n",
            "training loss: 7.030926704406738\n",
            "training loss: 7.1285319328308105\n",
            "training loss: 7.280974864959717\n",
            "training loss: 7.059499740600586\n",
            "training loss: 7.03983211517334\n",
            "training loss: 6.9199299812316895\n",
            "training loss: 6.9480133056640625\n",
            "training loss: 6.860259056091309\n",
            "training loss: 6.857054710388184\n",
            "training loss: 6.9767961502075195\n",
            "training loss: 6.911898136138916\n",
            "training loss: 6.656527519226074\n",
            "training loss: 6.878025054931641\n",
            "training loss: 6.914861679077148\n",
            "training loss: 6.827174186706543\n",
            "training loss: 6.804490089416504\n",
            "training loss: 6.909631252288818\n",
            "training loss: 7.1699371337890625\n",
            "training loss: 6.952138900756836\n",
            "training loss: 6.901837348937988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  32%|███▏      | 19/60 [11:32<24:13, 35.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.86376953125\n",
            "training loss: 7.032188415527344\n",
            "training loss: 7.311826705932617\n",
            "training loss: 7.108077526092529\n",
            "training loss: 7.078483581542969\n",
            "training loss: 7.137650012969971\n",
            "training loss: 7.016883850097656\n",
            "training loss: 6.893244743347168\n",
            "training loss: 6.967060089111328\n",
            "training loss: 7.0504608154296875\n",
            "training loss: 7.087912559509277\n",
            "training loss: 7.071584701538086\n",
            "training loss: 7.098933219909668\n",
            "training loss: 7.440584659576416\n",
            "training loss: 7.213421821594238\n",
            "training loss: 7.243375778198242\n",
            "training loss: 6.843379020690918\n",
            "training loss: 6.9501953125\n",
            "training loss: 6.963016986846924\n",
            "training loss: 6.611618995666504\n",
            "training loss: 6.872529029846191\n",
            "training loss: 6.876591205596924\n",
            "training loss: 7.081393718719482\n",
            "training loss: 6.951269149780273\n",
            "training loss: 7.150176048278809\n",
            "training loss: 6.819965362548828\n",
            "training loss: 6.958290100097656\n",
            "training loss: 6.699954032897949\n",
            "training loss: 6.699238300323486\n",
            "training loss: 6.987841606140137\n",
            "training loss: 6.9146599769592285\n",
            "training loss: 7.056720733642578\n",
            "training loss: 6.790053367614746\n",
            "training loss: 6.944843769073486\n",
            "training loss: 6.694242477416992\n",
            "training loss: 6.945778846740723\n",
            "training loss: 6.796910285949707\n",
            "training loss: 6.56756067276001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  33%|███▎      | 20/60 [12:07<23:33, 35.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.928761005401611\n",
            "training loss: 7.286394119262695\n",
            "training loss: 7.023438930511475\n",
            "training loss: 7.079920291900635\n",
            "training loss: 6.860331058502197\n",
            "training loss: 6.88519811630249\n",
            "training loss: 7.008325576782227\n",
            "training loss: 7.065738677978516\n",
            "training loss: 6.943793773651123\n",
            "training loss: 7.073919773101807\n",
            "training loss: 7.062038421630859\n",
            "training loss: 7.0735931396484375\n",
            "training loss: 7.022764682769775\n",
            "training loss: 6.9752960205078125\n",
            "training loss: 6.989070415496826\n",
            "training loss: 6.855942726135254\n",
            "training loss: 6.90767765045166\n",
            "training loss: 7.060669422149658\n",
            "training loss: 7.0186333656311035\n",
            "training loss: 6.815277576446533\n",
            "training loss: 6.702330112457275\n",
            "training loss: 6.779341697692871\n",
            "training loss: 7.051822662353516\n",
            "training loss: 6.725589275360107\n",
            "training loss: 6.942493438720703\n",
            "training loss: 6.772207736968994\n",
            "training loss: 6.915791034698486\n",
            "training loss: 6.79317569732666\n",
            "training loss: 6.892115592956543\n",
            "training loss: 7.0130743980407715\n",
            "training loss: 6.734713077545166\n",
            "training loss: 6.833933353424072\n",
            "training loss: 6.922115325927734\n",
            "training loss: 6.830386161804199\n",
            "training loss: 6.747946739196777\n",
            "training loss: 7.001604080200195\n",
            "training loss: 6.91196346282959\n",
            "training loss: 6.866403579711914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  35%|███▌      | 21/60 [12:43<22:58, 35.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.13380765914917\n",
            "training loss: 7.043793678283691\n",
            "training loss: 6.851597785949707\n",
            "training loss: 6.838294982910156\n",
            "training loss: 6.951021194458008\n",
            "training loss: 6.980280876159668\n",
            "training loss: 6.785924911499023\n",
            "training loss: 6.910454750061035\n",
            "training loss: 6.544434547424316\n",
            "training loss: 6.704256534576416\n",
            "training loss: 6.882894515991211\n",
            "training loss: 6.847042560577393\n",
            "training loss: 6.587689399719238\n",
            "training loss: 7.124894142150879\n",
            "training loss: 6.955557823181152\n",
            "training loss: 6.895047187805176\n",
            "training loss: 6.735470771789551\n",
            "training loss: 6.8612871170043945\n",
            "training loss: 7.228932857513428\n",
            "training loss: 7.0539093017578125\n",
            "training loss: 7.052230358123779\n",
            "training loss: 7.2176737785339355\n",
            "training loss: 7.188361644744873\n",
            "training loss: 7.022771835327148\n",
            "training loss: 6.88619327545166\n",
            "training loss: 6.955105781555176\n",
            "training loss: 6.509918212890625\n",
            "training loss: 6.953129291534424\n",
            "training loss: 6.686618804931641\n",
            "training loss: 6.700111389160156\n",
            "training loss: 6.665843963623047\n",
            "training loss: 6.911728858947754\n",
            "training loss: 6.906763553619385\n",
            "training loss: 6.969721794128418\n",
            "training loss: 6.935892105102539\n",
            "training loss: 7.233466625213623\n",
            "training loss: 6.7583112716674805\n",
            "training loss: 6.753925800323486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  37%|███▋      | 22/60 [13:18<22:26, 35.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.176062107086182\n",
            "training loss: 7.2588725090026855\n",
            "training loss: 7.11555814743042\n",
            "training loss: 6.925989151000977\n",
            "training loss: 6.908967018127441\n",
            "training loss: 6.987761974334717\n",
            "training loss: 6.810710906982422\n",
            "training loss: 6.8951334953308105\n",
            "training loss: 6.958134651184082\n",
            "training loss: 6.667666435241699\n",
            "training loss: 6.79388427734375\n",
            "training loss: 6.7920732498168945\n",
            "training loss: 6.862858772277832\n",
            "training loss: 6.858659744262695\n",
            "training loss: 7.008880138397217\n",
            "training loss: 6.934406280517578\n",
            "training loss: 6.691040515899658\n",
            "training loss: 6.881490230560303\n",
            "training loss: 7.233304500579834\n",
            "training loss: 7.241084098815918\n",
            "training loss: 7.040927886962891\n",
            "training loss: 7.141039848327637\n",
            "training loss: 6.740883827209473\n",
            "training loss: 6.959907531738281\n",
            "training loss: 6.732606410980225\n",
            "training loss: 6.971856594085693\n",
            "training loss: 6.9172282218933105\n",
            "training loss: 7.114466667175293\n",
            "training loss: 6.980304718017578\n",
            "training loss: 6.953433036804199\n",
            "training loss: 6.67807674407959\n",
            "training loss: 6.830268383026123\n",
            "training loss: 6.92100715637207\n",
            "training loss: 6.8055853843688965\n",
            "training loss: 6.694944858551025\n",
            "training loss: 6.869849681854248\n",
            "training loss: 6.713481903076172\n",
            "training loss: 6.600527763366699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  38%|███▊      | 23/60 [13:56<22:15, 36.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.9483642578125\n",
            "training loss: 6.989329814910889\n",
            "training loss: 7.078156471252441\n",
            "training loss: 7.0111284255981445\n",
            "training loss: 7.034083366394043\n",
            "training loss: 7.126873970031738\n",
            "training loss: 7.277239799499512\n",
            "training loss: 6.933209419250488\n",
            "training loss: 7.074723243713379\n",
            "training loss: 7.106433391571045\n",
            "training loss: 7.1303815841674805\n",
            "training loss: 6.9577317237854\n",
            "training loss: 7.125943660736084\n",
            "training loss: 6.951300144195557\n",
            "training loss: 7.037062168121338\n",
            "training loss: 7.121945381164551\n",
            "training loss: 7.177255630493164\n",
            "training loss: 7.18583345413208\n",
            "training loss: 7.11940860748291\n",
            "training loss: 6.690697193145752\n",
            "training loss: 7.032101154327393\n",
            "training loss: 6.948359966278076\n",
            "training loss: 6.792453765869141\n",
            "training loss: 6.794750213623047\n",
            "training loss: 7.012462615966797\n",
            "training loss: 6.497159481048584\n",
            "training loss: 7.033962249755859\n",
            "training loss: 7.09644889831543\n",
            "training loss: 6.870018005371094\n",
            "training loss: 6.918898105621338\n",
            "training loss: 6.841448783874512\n",
            "training loss: 6.700095176696777\n",
            "training loss: 6.733010292053223\n",
            "training loss: 6.738722324371338\n",
            "training loss: 6.715831279754639\n",
            "training loss: 7.180324077606201\n",
            "training loss: 7.116847991943359\n",
            "training loss: 6.886374473571777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  40%|████      | 24/60 [14:32<21:40, 36.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.438284873962402\n",
            "training loss: 7.244237899780273\n",
            "training loss: 7.0379638671875\n",
            "training loss: 7.103536128997803\n",
            "training loss: 7.0162200927734375\n",
            "training loss: 7.071226596832275\n",
            "training loss: 6.843860149383545\n",
            "training loss: 6.832047462463379\n",
            "training loss: 6.9487409591674805\n",
            "training loss: 7.123755931854248\n",
            "training loss: 6.802458763122559\n",
            "training loss: 6.762259483337402\n",
            "training loss: 6.7993597984313965\n",
            "training loss: 6.780742645263672\n",
            "training loss: 6.937795639038086\n",
            "training loss: 6.984611511230469\n",
            "training loss: 6.768003940582275\n",
            "training loss: 6.551722049713135\n",
            "training loss: 6.630376815795898\n",
            "training loss: 6.751323699951172\n",
            "training loss: 6.951770782470703\n",
            "training loss: 6.749368190765381\n",
            "training loss: 6.768223762512207\n",
            "training loss: 6.729194641113281\n",
            "training loss: 6.672029495239258\n",
            "training loss: 6.6809186935424805\n",
            "training loss: 6.814601898193359\n",
            "training loss: 6.9265947341918945\n",
            "training loss: 6.776663780212402\n",
            "training loss: 6.808378219604492\n",
            "training loss: 6.741751670837402\n",
            "training loss: 6.747103691101074\n",
            "training loss: 6.867014408111572\n",
            "training loss: 6.950596809387207\n",
            "training loss: 6.714591979980469\n",
            "training loss: 6.4913530349731445\n",
            "training loss: 6.6741862297058105\n",
            "training loss: 6.791191101074219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  42%|████▏     | 25/60 [15:09<21:11, 36.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.414606094360352\n",
            "training loss: 7.393224716186523\n",
            "training loss: 7.217447757720947\n",
            "training loss: 7.153511047363281\n",
            "training loss: 7.091983795166016\n",
            "training loss: 7.2212724685668945\n",
            "training loss: 7.088314056396484\n",
            "training loss: 7.040968418121338\n",
            "training loss: 7.015676498413086\n",
            "training loss: 6.909468650817871\n",
            "training loss: 7.100306510925293\n",
            "training loss: 6.842656135559082\n",
            "training loss: 7.073910713195801\n",
            "training loss: 6.884284973144531\n",
            "training loss: 7.040094375610352\n",
            "training loss: 7.0326995849609375\n",
            "training loss: 7.027273178100586\n",
            "training loss: 7.03261661529541\n",
            "training loss: 6.864243984222412\n",
            "training loss: 7.131856918334961\n",
            "training loss: 6.827532768249512\n",
            "training loss: 6.7283735275268555\n",
            "training loss: 6.853793144226074\n",
            "training loss: 6.7419939041137695\n",
            "training loss: 6.6175360679626465\n",
            "training loss: 6.873987197875977\n",
            "training loss: 6.918491840362549\n",
            "training loss: 6.638440132141113\n",
            "training loss: 6.960448265075684\n",
            "training loss: 6.81225061416626\n",
            "training loss: 6.754617214202881\n",
            "training loss: 6.850610733032227\n",
            "training loss: 6.801083564758301\n",
            "training loss: 6.922364234924316\n",
            "training loss: 7.030218124389648\n",
            "training loss: 6.829753875732422\n",
            "training loss: 6.740497589111328\n",
            "training loss: 6.623554229736328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  43%|████▎     | 26/60 [15:46<20:44, 36.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.934091567993164\n",
            "training loss: 6.843087673187256\n",
            "training loss: 6.729223251342773\n",
            "training loss: 6.805617332458496\n",
            "training loss: 6.700168609619141\n",
            "training loss: 6.841038703918457\n",
            "training loss: 6.911768913269043\n",
            "training loss: 6.988508701324463\n",
            "training loss: 7.206751823425293\n",
            "training loss: 7.142078876495361\n",
            "training loss: 6.903508186340332\n",
            "training loss: 7.0019636154174805\n",
            "training loss: 6.591575622558594\n",
            "training loss: 6.657236099243164\n",
            "training loss: 7.058627128601074\n",
            "training loss: 6.9093427658081055\n",
            "training loss: 6.87749719619751\n",
            "training loss: 6.8354644775390625\n",
            "training loss: 6.856874942779541\n",
            "training loss: 6.803351879119873\n",
            "training loss: 6.701775074005127\n",
            "training loss: 6.792572021484375\n",
            "training loss: 6.6465654373168945\n",
            "training loss: 7.004705429077148\n",
            "training loss: 6.6817522048950195\n",
            "training loss: 6.712252616882324\n",
            "training loss: 6.705495357513428\n",
            "training loss: 6.6070780754089355\n",
            "training loss: 6.84821081161499\n",
            "training loss: 6.69024658203125\n",
            "training loss: 7.017397880554199\n",
            "training loss: 7.188135147094727\n",
            "training loss: 7.0023112297058105\n",
            "training loss: 6.620026588439941\n",
            "training loss: 6.888774871826172\n",
            "training loss: 6.808568000793457\n",
            "training loss: 6.860589027404785\n",
            "training loss: 6.756828308105469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  45%|████▌     | 27/60 [16:23<20:09, 36.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.124218463897705\n",
            "training loss: 7.143487930297852\n",
            "training loss: 6.839852333068848\n",
            "training loss: 7.0654706954956055\n",
            "training loss: 7.28050422668457\n",
            "training loss: 7.1183552742004395\n",
            "training loss: 7.106115818023682\n",
            "training loss: 6.989500045776367\n",
            "training loss: 7.215797424316406\n",
            "training loss: 7.357195854187012\n",
            "training loss: 7.199697494506836\n",
            "training loss: 7.051196098327637\n",
            "training loss: 7.152863502502441\n",
            "training loss: 7.031560897827148\n",
            "training loss: 6.923406600952148\n",
            "training loss: 6.870909690856934\n",
            "training loss: 6.858661651611328\n",
            "training loss: 7.018649101257324\n",
            "training loss: 7.092916488647461\n",
            "training loss: 7.301746368408203\n",
            "training loss: 6.832308292388916\n",
            "training loss: 7.045387268066406\n",
            "training loss: 6.832042217254639\n",
            "training loss: 6.929388523101807\n",
            "training loss: 7.1596150398254395\n",
            "training loss: 6.895121097564697\n",
            "training loss: 6.873267650604248\n",
            "training loss: 6.801031589508057\n",
            "training loss: 6.937482833862305\n",
            "training loss: 6.916628360748291\n",
            "training loss: 6.855363845825195\n",
            "training loss: 7.051529407501221\n",
            "training loss: 6.902165412902832\n",
            "training loss: 6.79555606842041\n",
            "training loss: 6.814321517944336\n",
            "training loss: 6.762935161590576\n",
            "training loss: 6.805954933166504\n",
            "training loss: 6.738912582397461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  47%|████▋     | 28/60 [17:00<19:36, 36.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.947281837463379\n",
            "training loss: 7.211562156677246\n",
            "training loss: 7.247596740722656\n",
            "training loss: 7.042947292327881\n",
            "training loss: 7.22401237487793\n",
            "training loss: 7.223123550415039\n",
            "training loss: 7.040452003479004\n",
            "training loss: 7.249017238616943\n",
            "training loss: 6.895151615142822\n",
            "training loss: 6.892417907714844\n",
            "training loss: 7.295907497406006\n",
            "training loss: 7.1659698486328125\n",
            "training loss: 7.200960159301758\n",
            "training loss: 7.018048286437988\n",
            "training loss: 6.876317024230957\n",
            "training loss: 6.804475784301758\n",
            "training loss: 6.6895751953125\n",
            "training loss: 6.729691505432129\n",
            "training loss: 6.915064811706543\n",
            "training loss: 6.851601600646973\n",
            "training loss: 6.973831653594971\n",
            "training loss: 6.729957580566406\n",
            "training loss: 7.056665420532227\n",
            "training loss: 6.74712610244751\n",
            "training loss: 6.82427978515625\n",
            "training loss: 7.023138999938965\n",
            "training loss: 6.948177337646484\n",
            "training loss: 7.064384460449219\n",
            "training loss: 6.96017599105835\n",
            "training loss: 6.9643049240112305\n",
            "training loss: 6.797094345092773\n",
            "training loss: 6.868595123291016\n",
            "training loss: 7.003716468811035\n",
            "training loss: 6.726412296295166\n",
            "training loss: 6.578341960906982\n",
            "training loss: 6.843327522277832\n",
            "training loss: 6.887835502624512\n",
            "training loss: 6.756780624389648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  48%|████▊     | 29/60 [17:37<18:57, 36.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.153345108032227\n",
            "training loss: 7.046444892883301\n",
            "training loss: 7.11208438873291\n",
            "training loss: 7.008672714233398\n",
            "training loss: 6.9976301193237305\n",
            "training loss: 6.887163162231445\n",
            "training loss: 6.988776206970215\n",
            "training loss: 6.825563907623291\n",
            "training loss: 7.07639217376709\n",
            "training loss: 6.98415470123291\n",
            "training loss: 6.812728404998779\n",
            "training loss: 6.976704120635986\n",
            "training loss: 6.973231315612793\n",
            "training loss: 7.055208683013916\n",
            "training loss: 7.002471923828125\n",
            "training loss: 6.856413841247559\n",
            "training loss: 6.817113399505615\n",
            "training loss: 6.822014808654785\n",
            "training loss: 6.930717468261719\n",
            "training loss: 6.982941627502441\n",
            "training loss: 7.006173610687256\n",
            "training loss: 6.909756660461426\n",
            "training loss: 6.563672065734863\n",
            "training loss: 6.914226531982422\n",
            "training loss: 6.758958339691162\n",
            "training loss: 6.676558494567871\n",
            "training loss: 6.906651496887207\n",
            "training loss: 6.993564605712891\n",
            "training loss: 6.663363933563232\n",
            "training loss: 6.7737274169921875\n",
            "training loss: 6.946592807769775\n",
            "training loss: 6.684568881988525\n",
            "training loss: 6.8698225021362305\n",
            "training loss: 6.699105262756348\n",
            "training loss: 6.9838786125183105\n",
            "training loss: 6.802493572235107\n",
            "training loss: 6.778954029083252\n",
            "training loss: 6.781511306762695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  50%|█████     | 30/60 [18:21<19:33, 39.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.815919876098633\n",
            "training loss: 6.752330780029297\n",
            "training loss: 6.71934700012207\n",
            "training loss: 6.925662040710449\n",
            "training loss: 6.714179992675781\n",
            "training loss: 6.7853875160217285\n",
            "training loss: 6.933854103088379\n",
            "training loss: 7.069599628448486\n",
            "training loss: 6.915974140167236\n",
            "training loss: 6.899112701416016\n",
            "training loss: 6.999452590942383\n",
            "training loss: 6.9288787841796875\n",
            "training loss: 7.18569278717041\n",
            "training loss: 7.157340049743652\n",
            "training loss: 6.868141174316406\n",
            "training loss: 6.888795375823975\n",
            "training loss: 6.974389553070068\n",
            "training loss: 6.783537864685059\n",
            "training loss: 7.142969608306885\n",
            "training loss: 7.311019420623779\n",
            "training loss: 7.179962158203125\n",
            "training loss: 7.216696262359619\n",
            "training loss: 7.037586212158203\n",
            "training loss: 6.982458114624023\n",
            "training loss: 6.819932460784912\n",
            "training loss: 6.79979944229126\n",
            "training loss: 6.753216743469238\n",
            "training loss: 6.760767936706543\n",
            "training loss: 6.898104190826416\n",
            "training loss: 6.970061779022217\n",
            "training loss: 7.0470290184021\n",
            "training loss: 7.154153823852539\n",
            "training loss: 6.798857688903809\n",
            "training loss: 6.79429817199707\n",
            "training loss: 6.854142189025879\n",
            "training loss: 7.0940327644348145\n",
            "training loss: 6.8156304359436035\n",
            "training loss: 6.841958999633789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  52%|█████▏    | 31/60 [19:01<18:56, 39.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.325046062469482\n",
            "training loss: 7.202572822570801\n",
            "training loss: 7.225522518157959\n",
            "training loss: 6.889896392822266\n",
            "training loss: 7.090451240539551\n",
            "training loss: 6.883677959442139\n",
            "training loss: 7.195692539215088\n",
            "training loss: 7.121579647064209\n",
            "training loss: 6.933065891265869\n",
            "training loss: 7.022035598754883\n",
            "training loss: 7.005949974060059\n",
            "training loss: 6.975170612335205\n",
            "training loss: 6.752547740936279\n",
            "training loss: 6.828690528869629\n",
            "training loss: 6.867093563079834\n",
            "training loss: 6.971599578857422\n",
            "training loss: 6.927452564239502\n",
            "training loss: 6.899593830108643\n",
            "training loss: 7.1784749031066895\n",
            "training loss: 7.181918144226074\n",
            "training loss: 6.890713214874268\n",
            "training loss: 7.049166679382324\n",
            "training loss: 6.936261177062988\n",
            "training loss: 6.916055679321289\n",
            "training loss: 6.74778413772583\n",
            "training loss: 7.003689765930176\n",
            "training loss: 6.522190093994141\n",
            "training loss: 7.017594814300537\n",
            "training loss: 6.886348247528076\n",
            "training loss: 6.937753677368164\n",
            "training loss: 7.093332290649414\n",
            "training loss: 7.063557147979736\n",
            "training loss: 6.8260016441345215\n",
            "training loss: 6.881526947021484\n",
            "training loss: 6.906803131103516\n",
            "training loss: 7.024253845214844\n",
            "training loss: 6.948357105255127\n",
            "training loss: 6.995831489562988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  53%|█████▎    | 32/60 [19:38<18:01, 38.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.150744915008545\n",
            "training loss: 6.8194355964660645\n",
            "training loss: 7.041787147521973\n",
            "training loss: 6.919503211975098\n",
            "training loss: 7.009572982788086\n",
            "training loss: 7.15895938873291\n",
            "training loss: 6.872072219848633\n",
            "training loss: 6.811233043670654\n",
            "training loss: 6.922792434692383\n",
            "training loss: 7.186446189880371\n",
            "training loss: 7.249463081359863\n",
            "training loss: 6.931093215942383\n",
            "training loss: 7.050326824188232\n",
            "training loss: 7.2291460037231445\n",
            "training loss: 7.294283866882324\n",
            "training loss: 7.005265712738037\n",
            "training loss: 7.088460922241211\n",
            "training loss: 6.760744094848633\n",
            "training loss: 6.983546733856201\n",
            "training loss: 6.899620056152344\n",
            "training loss: 6.783875465393066\n",
            "training loss: 6.775445938110352\n",
            "training loss: 6.850049018859863\n",
            "training loss: 6.7378058433532715\n",
            "training loss: 6.716808795928955\n",
            "training loss: 6.808932781219482\n",
            "training loss: 6.898334980010986\n",
            "training loss: 7.011486053466797\n",
            "training loss: 6.937963485717773\n",
            "training loss: 6.982948303222656\n",
            "training loss: 6.902017593383789\n",
            "training loss: 6.822504043579102\n",
            "training loss: 7.093039512634277\n",
            "training loss: 6.90128231048584\n",
            "training loss: 7.0124616622924805\n",
            "training loss: 6.987841606140137\n",
            "training loss: 6.832006454467773\n",
            "training loss: 6.8645501136779785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  55%|█████▌    | 33/60 [20:19<17:46, 39.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.096930503845215\n",
            "training loss: 6.917208671569824\n",
            "training loss: 6.889307975769043\n",
            "training loss: 6.72829532623291\n",
            "training loss: 7.00083589553833\n",
            "training loss: 7.0845866203308105\n",
            "training loss: 7.125032901763916\n",
            "training loss: 7.08589506149292\n",
            "training loss: 6.887714385986328\n",
            "training loss: 6.870408058166504\n",
            "training loss: 6.79323148727417\n",
            "training loss: 6.701129913330078\n",
            "training loss: 6.8646559715271\n",
            "training loss: 6.588942527770996\n",
            "training loss: 6.914673805236816\n",
            "training loss: 6.979317665100098\n",
            "training loss: 7.106926918029785\n",
            "training loss: 7.068445205688477\n",
            "training loss: 6.952410697937012\n",
            "training loss: 6.732590198516846\n",
            "training loss: 6.801033973693848\n",
            "training loss: 6.849584102630615\n",
            "training loss: 6.7504096031188965\n",
            "training loss: 6.876327037811279\n",
            "training loss: 6.736177444458008\n",
            "training loss: 6.7564263343811035\n",
            "training loss: 6.987931251525879\n",
            "training loss: 7.005263328552246\n",
            "training loss: 6.662690162658691\n",
            "training loss: 6.7080888748168945\n",
            "training loss: 6.814984321594238\n",
            "training loss: 6.861089706420898\n",
            "training loss: 6.761648654937744\n",
            "training loss: 6.961475372314453\n",
            "training loss: 6.835525989532471\n",
            "training loss: 6.837425708770752\n",
            "training loss: 6.6617937088012695\n",
            "training loss: 6.675411224365234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  57%|█████▋    | 34/60 [20:57<16:51, 38.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.062642574310303\n",
            "training loss: 6.713803291320801\n",
            "training loss: 7.109666347503662\n",
            "training loss: 6.973330974578857\n",
            "training loss: 6.710875511169434\n",
            "training loss: 6.854310035705566\n",
            "training loss: 6.948807716369629\n",
            "training loss: 6.87269401550293\n",
            "training loss: 6.770455360412598\n",
            "training loss: 6.809700012207031\n",
            "training loss: 6.9551591873168945\n",
            "training loss: 6.771195411682129\n",
            "training loss: 6.559839248657227\n",
            "training loss: 6.889672756195068\n",
            "training loss: 6.865398406982422\n",
            "training loss: 6.988015174865723\n",
            "training loss: 6.641311168670654\n",
            "training loss: 6.811058521270752\n",
            "training loss: 6.755307674407959\n",
            "training loss: 6.788395881652832\n",
            "training loss: 7.063396453857422\n",
            "training loss: 6.94667387008667\n",
            "training loss: 6.770275115966797\n",
            "training loss: 6.789013385772705\n",
            "training loss: 6.756941795349121\n",
            "training loss: 6.512997627258301\n",
            "training loss: 6.749382019042969\n",
            "training loss: 6.883901119232178\n",
            "training loss: 6.832156181335449\n",
            "training loss: 6.423586845397949\n",
            "training loss: 6.5444440841674805\n",
            "training loss: 6.397632598876953\n",
            "training loss: 6.563011169433594\n",
            "training loss: 6.893718719482422\n",
            "training loss: 6.745124816894531\n",
            "training loss: 6.785486698150635\n",
            "training loss: 6.693463325500488\n",
            "training loss: 6.712929725646973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  58%|█████▊    | 35/60 [21:36<16:10, 38.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.963165283203125\n",
            "training loss: 7.011050224304199\n",
            "training loss: 6.9451141357421875\n",
            "training loss: 6.980106830596924\n",
            "training loss: 6.79909610748291\n",
            "training loss: 7.128789901733398\n",
            "training loss: 6.949100494384766\n",
            "training loss: 7.0172224044799805\n",
            "training loss: 6.679032325744629\n",
            "training loss: 7.110471248626709\n",
            "training loss: 7.086721420288086\n",
            "training loss: 7.127693176269531\n",
            "training loss: 7.124792575836182\n",
            "training loss: 6.949446201324463\n",
            "training loss: 7.067544460296631\n",
            "training loss: 6.976029396057129\n",
            "training loss: 6.881223201751709\n",
            "training loss: 6.907029628753662\n",
            "training loss: 6.898005485534668\n",
            "training loss: 6.877988815307617\n",
            "training loss: 6.981573581695557\n",
            "training loss: 6.9188737869262695\n",
            "training loss: 7.0098090171813965\n",
            "training loss: 6.697917938232422\n",
            "training loss: 6.835507869720459\n",
            "training loss: 6.816393852233887\n",
            "training loss: 6.8620171546936035\n",
            "training loss: 6.835174560546875\n",
            "training loss: 6.9162163734436035\n",
            "training loss: 6.760327339172363\n",
            "training loss: 6.812173366546631\n",
            "training loss: 6.732647895812988\n",
            "training loss: 6.837010860443115\n",
            "training loss: 6.910126209259033\n",
            "training loss: 7.08862829208374\n",
            "training loss: 6.764854431152344\n",
            "training loss: 6.749780178070068\n",
            "training loss: 6.656655311584473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  60%|██████    | 36/60 [22:15<15:34, 38.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.104833602905273\n",
            "training loss: 7.227715492248535\n",
            "training loss: 7.2382588386535645\n",
            "training loss: 7.062765121459961\n",
            "training loss: 7.265451431274414\n",
            "training loss: 7.126526832580566\n",
            "training loss: 6.827349662780762\n",
            "training loss: 7.055377960205078\n",
            "training loss: 6.969204902648926\n",
            "training loss: 6.782235145568848\n",
            "training loss: 7.10551643371582\n",
            "training loss: 7.042754173278809\n",
            "training loss: 6.819971561431885\n",
            "training loss: 6.680753707885742\n",
            "training loss: 7.018661975860596\n",
            "training loss: 6.773958206176758\n",
            "training loss: 6.83604097366333\n",
            "training loss: 6.661881923675537\n",
            "training loss: 6.381363868713379\n",
            "training loss: 6.952376365661621\n",
            "training loss: 6.737213134765625\n",
            "training loss: 6.827164649963379\n",
            "training loss: 6.845854759216309\n",
            "training loss: 6.923000335693359\n",
            "training loss: 6.774203300476074\n",
            "training loss: 6.8379364013671875\n",
            "training loss: 6.907336711883545\n",
            "training loss: 6.507006645202637\n",
            "training loss: 6.523191452026367\n",
            "training loss: 6.463077545166016\n",
            "training loss: 6.581181526184082\n",
            "training loss: 6.777015686035156\n",
            "training loss: 6.873549461364746\n",
            "training loss: 6.262451171875\n",
            "training loss: 6.568530082702637\n",
            "training loss: 6.86988639831543\n",
            "training loss: 6.487429618835449\n",
            "training loss: 6.860185623168945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  62%|██████▏   | 37/60 [22:53<14:49, 38.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.9352264404296875\n",
            "training loss: 6.93167781829834\n",
            "training loss: 7.2708563804626465\n",
            "training loss: 7.05967903137207\n",
            "training loss: 7.069145679473877\n",
            "training loss: 7.159997940063477\n",
            "training loss: 6.854372024536133\n",
            "training loss: 6.9243669509887695\n",
            "training loss: 6.986872673034668\n",
            "training loss: 7.018465042114258\n",
            "training loss: 6.817232131958008\n",
            "training loss: 6.587927341461182\n",
            "training loss: 6.764934062957764\n",
            "training loss: 6.897608757019043\n",
            "training loss: 7.027773857116699\n",
            "training loss: 6.864953994750977\n",
            "training loss: 6.794806003570557\n",
            "training loss: 6.743534088134766\n",
            "training loss: 6.842552185058594\n",
            "training loss: 6.820154190063477\n",
            "training loss: 6.838502883911133\n",
            "training loss: 6.606867790222168\n",
            "training loss: 6.613940715789795\n",
            "training loss: 6.84549617767334\n",
            "training loss: 6.636460304260254\n",
            "training loss: 6.836737155914307\n",
            "training loss: 6.6534271240234375\n",
            "training loss: 6.663595676422119\n",
            "training loss: 6.580119609832764\n",
            "training loss: 6.546696662902832\n",
            "training loss: 6.713649749755859\n",
            "training loss: 6.812425136566162\n",
            "training loss: 6.69025182723999\n",
            "training loss: 6.369051456451416\n",
            "training loss: 6.795965194702148\n",
            "training loss: 6.722864151000977\n",
            "training loss: 6.925552845001221\n",
            "training loss: 7.179100513458252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  63%|██████▎   | 38/60 [23:30<13:57, 38.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.91759729385376\n",
            "training loss: 6.880820274353027\n",
            "training loss: 6.802462577819824\n",
            "training loss: 6.969445705413818\n",
            "training loss: 6.908226490020752\n",
            "training loss: 7.049434185028076\n",
            "training loss: 7.048873424530029\n",
            "training loss: 6.941977024078369\n",
            "training loss: 7.061935901641846\n",
            "training loss: 6.828742504119873\n",
            "training loss: 6.928763389587402\n",
            "training loss: 6.875249862670898\n",
            "training loss: 6.7576189041137695\n",
            "training loss: 7.081170558929443\n",
            "training loss: 6.946403980255127\n",
            "training loss: 7.2863359451293945\n",
            "training loss: 7.084437370300293\n",
            "training loss: 7.072866916656494\n",
            "training loss: 6.762518405914307\n",
            "training loss: 6.728160381317139\n",
            "training loss: 7.067739486694336\n",
            "training loss: 6.75382137298584\n",
            "training loss: 7.054401874542236\n",
            "training loss: 6.788276672363281\n",
            "training loss: 6.775176048278809\n",
            "training loss: 6.967655181884766\n",
            "training loss: 6.905828952789307\n",
            "training loss: 7.169620990753174\n",
            "training loss: 6.718811988830566\n",
            "training loss: 6.791036605834961\n",
            "training loss: 6.787951469421387\n",
            "training loss: 6.724164962768555\n",
            "training loss: 6.735098361968994\n",
            "training loss: 6.969395160675049\n",
            "training loss: 7.001649856567383\n",
            "training loss: 6.774174690246582\n",
            "training loss: 6.8095808029174805\n",
            "training loss: 6.714784145355225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  65%|██████▌   | 39/60 [24:10<13:32, 38.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.190919876098633\n",
            "training loss: 7.104879379272461\n",
            "training loss: 6.972055435180664\n",
            "training loss: 6.990058898925781\n",
            "training loss: 6.711068153381348\n",
            "training loss: 6.839095115661621\n",
            "training loss: 6.66373348236084\n",
            "training loss: 6.960134506225586\n",
            "training loss: 6.890723705291748\n",
            "training loss: 6.933787822723389\n",
            "training loss: 7.002847671508789\n",
            "training loss: 7.072235107421875\n",
            "training loss: 6.626645565032959\n",
            "training loss: 6.76505184173584\n",
            "training loss: 6.995948791503906\n",
            "training loss: 6.750307083129883\n",
            "training loss: 6.872686386108398\n",
            "training loss: 6.970120429992676\n",
            "training loss: 6.849707126617432\n",
            "training loss: 7.054137706756592\n",
            "training loss: 7.114562034606934\n",
            "training loss: 6.811920166015625\n",
            "training loss: 6.8133649826049805\n",
            "training loss: 6.865438461303711\n",
            "training loss: 6.992670059204102\n",
            "training loss: 7.040085315704346\n",
            "training loss: 7.132332801818848\n",
            "training loss: 7.178670406341553\n",
            "training loss: 7.088171005249023\n",
            "training loss: 6.945948123931885\n",
            "training loss: 6.953282356262207\n",
            "training loss: 6.760034561157227\n",
            "training loss: 6.922534942626953\n",
            "training loss: 7.202908515930176\n",
            "training loss: 7.196171760559082\n",
            "training loss: 7.037339210510254\n",
            "training loss: 6.839658737182617\n",
            "training loss: 6.5843658447265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  67%|██████▋   | 40/60 [24:50<13:02, 39.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.998033046722412\n",
            "training loss: 7.1120758056640625\n",
            "training loss: 7.058141708374023\n",
            "training loss: 7.13704776763916\n",
            "training loss: 6.829887390136719\n",
            "training loss: 7.1608710289001465\n",
            "training loss: 7.175714492797852\n",
            "training loss: 6.901777267456055\n",
            "training loss: 6.836485862731934\n",
            "training loss: 6.8558735847473145\n",
            "training loss: 6.702122688293457\n",
            "training loss: 6.836633205413818\n",
            "training loss: 6.952167510986328\n",
            "training loss: 6.795049667358398\n",
            "training loss: 7.096199989318848\n",
            "training loss: 7.227014541625977\n",
            "training loss: 7.3196702003479\n",
            "training loss: 7.175261497497559\n",
            "training loss: 7.238480567932129\n",
            "training loss: 7.084270477294922\n",
            "training loss: 7.160463333129883\n",
            "training loss: 7.111458778381348\n",
            "training loss: 7.244255065917969\n",
            "training loss: 7.105845928192139\n",
            "training loss: 6.808102607727051\n",
            "training loss: 7.040849685668945\n",
            "training loss: 7.130255699157715\n",
            "training loss: 7.033421516418457\n",
            "training loss: 7.0296149253845215\n",
            "training loss: 7.064847946166992\n",
            "training loss: 6.939940452575684\n",
            "training loss: 7.00942325592041\n",
            "training loss: 7.19626522064209\n",
            "training loss: 6.916988849639893\n",
            "training loss: 6.870460033416748\n",
            "training loss: 6.721942901611328\n",
            "training loss: 6.8807373046875\n",
            "training loss: 6.863302230834961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  68%|██████▊   | 41/60 [25:29<12:23, 39.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.022434234619141\n",
            "training loss: 7.035321235656738\n",
            "training loss: 7.104270935058594\n",
            "training loss: 7.024728775024414\n",
            "training loss: 7.103756427764893\n",
            "training loss: 7.121591567993164\n",
            "training loss: 6.9495768547058105\n",
            "training loss: 6.845346450805664\n",
            "training loss: 6.982026100158691\n",
            "training loss: 6.665581226348877\n",
            "training loss: 6.810197830200195\n",
            "training loss: 6.74412727355957\n",
            "training loss: 6.880705833435059\n",
            "training loss: 6.990229606628418\n",
            "training loss: 6.83351469039917\n",
            "training loss: 6.827588081359863\n",
            "training loss: 6.91723108291626\n",
            "training loss: 6.8456339836120605\n",
            "training loss: 7.001211643218994\n",
            "training loss: 7.027740955352783\n",
            "training loss: 7.050402641296387\n",
            "training loss: 6.859005451202393\n",
            "training loss: 6.85260009765625\n",
            "training loss: 6.802261829376221\n",
            "training loss: 6.9958600997924805\n",
            "training loss: 6.760618209838867\n",
            "training loss: 6.496265411376953\n",
            "training loss: 6.808382034301758\n",
            "training loss: 6.68642520904541\n",
            "training loss: 6.715137481689453\n",
            "training loss: 6.6345319747924805\n",
            "training loss: 6.632160186767578\n",
            "training loss: 6.856772422790527\n",
            "training loss: 6.567876815795898\n",
            "training loss: 6.798819065093994\n",
            "training loss: 6.872398376464844\n",
            "training loss: 6.9566650390625\n",
            "training loss: 6.728176116943359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  70%|███████   | 42/60 [26:08<11:43, 39.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.028550624847412\n",
            "training loss: 6.492809295654297\n",
            "training loss: 6.734537601470947\n",
            "training loss: 6.8820719718933105\n",
            "training loss: 7.137943744659424\n",
            "training loss: 7.164299011230469\n",
            "training loss: 7.263246536254883\n",
            "training loss: 6.87803840637207\n",
            "training loss: 6.9386396408081055\n",
            "training loss: 7.19969367980957\n",
            "training loss: 7.039068222045898\n",
            "training loss: 7.117816925048828\n",
            "training loss: 6.92111873626709\n",
            "training loss: 6.997204780578613\n",
            "training loss: 7.192662239074707\n",
            "training loss: 6.895604133605957\n",
            "training loss: 6.955484390258789\n",
            "training loss: 6.858512878417969\n",
            "training loss: 6.841434478759766\n",
            "training loss: 6.753368377685547\n",
            "training loss: 7.028791427612305\n",
            "training loss: 6.8934855461120605\n",
            "training loss: 6.793466091156006\n",
            "training loss: 6.894412994384766\n",
            "training loss: 6.76237154006958\n",
            "training loss: 6.819368362426758\n",
            "training loss: 6.873926162719727\n",
            "training loss: 6.7379961013793945\n",
            "training loss: 6.834006309509277\n",
            "training loss: 6.914346218109131\n",
            "training loss: 6.716160774230957\n",
            "training loss: 6.736766338348389\n",
            "training loss: 6.963287353515625\n",
            "training loss: 6.8481950759887695\n",
            "training loss: 6.769565105438232\n",
            "training loss: 6.851259231567383\n",
            "training loss: 6.895686626434326\n",
            "training loss: 6.89988899230957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  72%|███████▏  | 43/60 [26:47<11:05, 39.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.920917510986328\n",
            "training loss: 6.774284362792969\n",
            "training loss: 6.98546028137207\n",
            "training loss: 7.0495452880859375\n",
            "training loss: 7.069087028503418\n",
            "training loss: 7.227439880371094\n",
            "training loss: 7.163623809814453\n",
            "training loss: 7.163667678833008\n",
            "training loss: 7.071800231933594\n",
            "training loss: 7.168830394744873\n",
            "training loss: 7.084744930267334\n",
            "training loss: 6.7596564292907715\n",
            "training loss: 6.989879131317139\n",
            "training loss: 6.8432817459106445\n",
            "training loss: 6.979320049285889\n",
            "training loss: 6.766057968139648\n",
            "training loss: 6.911478519439697\n",
            "training loss: 6.894661903381348\n",
            "training loss: 6.869846343994141\n",
            "training loss: 6.663836479187012\n",
            "training loss: 6.712013244628906\n",
            "training loss: 6.813166618347168\n",
            "training loss: 6.950087547302246\n",
            "training loss: 6.793046474456787\n",
            "training loss: 6.893374919891357\n",
            "training loss: 6.895325660705566\n",
            "training loss: 6.667780876159668\n",
            "training loss: 6.967254638671875\n",
            "training loss: 6.771189212799072\n",
            "training loss: 6.857645511627197\n",
            "training loss: 6.745177745819092\n",
            "training loss: 6.724328994750977\n",
            "training loss: 6.776288032531738\n",
            "training loss: 7.024880886077881\n",
            "training loss: 7.168120384216309\n",
            "training loss: 6.902871131896973\n",
            "training loss: 7.076911926269531\n",
            "training loss: 7.010315895080566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  73%|███████▎  | 44/60 [27:29<10:37, 39.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.985262870788574\n",
            "training loss: 7.013186931610107\n",
            "training loss: 6.914541721343994\n",
            "training loss: 6.732997417449951\n",
            "training loss: 6.985907554626465\n",
            "training loss: 6.904106140136719\n",
            "training loss: 6.758103847503662\n",
            "training loss: 6.927319526672363\n",
            "training loss: 6.9725470542907715\n",
            "training loss: 6.884548187255859\n",
            "training loss: 7.0860137939453125\n",
            "training loss: 7.039617538452148\n",
            "training loss: 6.903032302856445\n",
            "training loss: 6.580796718597412\n",
            "training loss: 6.835909843444824\n",
            "training loss: 6.938174247741699\n",
            "training loss: 6.949499130249023\n",
            "training loss: 6.848749160766602\n",
            "training loss: 7.055150032043457\n",
            "training loss: 6.567164421081543\n",
            "training loss: 6.625057220458984\n",
            "training loss: 6.928713798522949\n",
            "training loss: 6.855886936187744\n",
            "training loss: 6.854824066162109\n",
            "training loss: 6.766557693481445\n",
            "training loss: 6.759390830993652\n",
            "training loss: 6.88419246673584\n",
            "training loss: 6.875560760498047\n",
            "training loss: 6.684985160827637\n",
            "training loss: 6.8350067138671875\n",
            "training loss: 6.654313087463379\n",
            "training loss: 6.975523948669434\n",
            "training loss: 6.735792636871338\n",
            "training loss: 6.684609413146973\n",
            "training loss: 6.975354194641113\n",
            "training loss: 6.789193153381348\n",
            "training loss: 6.8598198890686035\n",
            "training loss: 6.619948387145996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  75%|███████▌  | 45/60 [28:05<09:41, 38.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.833057880401611\n",
            "training loss: 7.051260471343994\n",
            "training loss: 6.930232524871826\n",
            "training loss: 6.8667988777160645\n",
            "training loss: 6.924149990081787\n",
            "training loss: 7.053192138671875\n",
            "training loss: 6.962886810302734\n",
            "training loss: 6.6574387550354\n",
            "training loss: 6.921233177185059\n",
            "training loss: 6.80277156829834\n",
            "training loss: 6.7086567878723145\n",
            "training loss: 6.827416896820068\n",
            "training loss: 7.037134170532227\n",
            "training loss: 6.655277729034424\n",
            "training loss: 7.0836873054504395\n",
            "training loss: 6.946837902069092\n",
            "training loss: 6.79092264175415\n",
            "training loss: 7.0138068199157715\n",
            "training loss: 6.853765964508057\n",
            "training loss: 6.900178909301758\n",
            "training loss: 6.755011081695557\n",
            "training loss: 6.829309463500977\n",
            "training loss: 6.910581588745117\n",
            "training loss: 6.771367073059082\n",
            "training loss: 6.773972034454346\n",
            "training loss: 6.745842456817627\n",
            "training loss: 6.784134864807129\n",
            "training loss: 7.165299415588379\n",
            "training loss: 6.850048065185547\n",
            "training loss: 6.875415802001953\n",
            "training loss: 6.666441440582275\n",
            "training loss: 6.639601707458496\n",
            "training loss: 6.781032085418701\n",
            "training loss: 6.76755428314209\n",
            "training loss: 6.401064872741699\n",
            "training loss: 6.475872993469238\n",
            "training loss: 6.767517566680908\n",
            "training loss: 6.5800065994262695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  77%|███████▋  | 46/60 [28:40<08:47, 37.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.849803924560547\n",
            "training loss: 6.868705749511719\n",
            "training loss: 7.014100074768066\n",
            "training loss: 6.884118556976318\n",
            "training loss: 7.096312522888184\n",
            "training loss: 7.044736385345459\n",
            "training loss: 6.923703193664551\n",
            "training loss: 6.827785491943359\n",
            "training loss: 6.782626152038574\n",
            "training loss: 6.838523864746094\n",
            "training loss: 7.021111488342285\n",
            "training loss: 7.082637786865234\n",
            "training loss: 7.014553070068359\n",
            "training loss: 6.671238422393799\n",
            "training loss: 6.666596412658691\n",
            "training loss: 6.801071643829346\n",
            "training loss: 6.602678298950195\n",
            "training loss: 6.865324020385742\n",
            "training loss: 6.746626377105713\n",
            "training loss: 6.768252849578857\n",
            "training loss: 7.006462574005127\n",
            "training loss: 6.912740707397461\n",
            "training loss: 7.034421920776367\n",
            "training loss: 6.945222854614258\n",
            "training loss: 6.846061706542969\n",
            "training loss: 6.8226094245910645\n",
            "training loss: 6.8805952072143555\n",
            "training loss: 6.828042984008789\n",
            "training loss: 6.850577354431152\n",
            "training loss: 6.528097152709961\n",
            "training loss: 6.809523582458496\n",
            "training loss: 6.685946464538574\n",
            "training loss: 6.78880500793457\n",
            "training loss: 6.8367414474487305\n",
            "training loss: 6.803589820861816\n",
            "training loss: 6.755095481872559\n",
            "training loss: 6.888038635253906\n",
            "training loss: 6.722182750701904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  78%|███████▊  | 47/60 [29:19<08:13, 37.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.015035629272461\n",
            "training loss: 7.010560989379883\n",
            "training loss: 7.241272449493408\n",
            "training loss: 6.928518295288086\n",
            "training loss: 7.037078857421875\n",
            "training loss: 7.065497398376465\n",
            "training loss: 7.001681327819824\n",
            "training loss: 6.793374061584473\n",
            "training loss: 6.930091857910156\n",
            "training loss: 7.14011287689209\n",
            "training loss: 7.0072221755981445\n",
            "training loss: 6.760674476623535\n",
            "training loss: 6.792230129241943\n",
            "training loss: 6.943887710571289\n",
            "training loss: 7.180753707885742\n",
            "training loss: 7.197806358337402\n",
            "training loss: 7.094391822814941\n",
            "training loss: 7.061016082763672\n",
            "training loss: 7.016717910766602\n",
            "training loss: 6.914093971252441\n",
            "training loss: 6.992034912109375\n",
            "training loss: 6.813416004180908\n",
            "training loss: 6.960330963134766\n",
            "training loss: 6.857560157775879\n",
            "training loss: 6.973033905029297\n",
            "training loss: 6.893651962280273\n",
            "training loss: 6.771756172180176\n",
            "training loss: 6.718382835388184\n",
            "training loss: 6.67213249206543\n",
            "training loss: 6.876820087432861\n",
            "training loss: 6.6978254318237305\n",
            "training loss: 6.773942947387695\n",
            "training loss: 6.8841447830200195\n",
            "training loss: 6.875450134277344\n",
            "training loss: 6.98567008972168\n",
            "training loss: 6.942199230194092\n",
            "training loss: 6.74237060546875\n",
            "training loss: 6.722424507141113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  80%|████████  | 48/60 [29:58<07:39, 38.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.078429698944092\n",
            "training loss: 6.9657673835754395\n",
            "training loss: 7.093679428100586\n",
            "training loss: 6.927056312561035\n",
            "training loss: 7.040732383728027\n",
            "training loss: 7.10649299621582\n",
            "training loss: 7.048852920532227\n",
            "training loss: 7.018520355224609\n",
            "training loss: 6.726898193359375\n",
            "training loss: 6.858676910400391\n",
            "training loss: 6.842465400695801\n",
            "training loss: 6.967240333557129\n",
            "training loss: 6.977772235870361\n",
            "training loss: 7.010385513305664\n",
            "training loss: 6.815104961395264\n",
            "training loss: 6.862213611602783\n",
            "training loss: 6.620127201080322\n",
            "training loss: 6.6861066818237305\n",
            "training loss: 6.844273090362549\n",
            "training loss: 6.877657890319824\n",
            "training loss: 6.860384941101074\n",
            "training loss: 6.868480682373047\n",
            "training loss: 6.981391906738281\n",
            "training loss: 6.8136091232299805\n",
            "training loss: 6.963067054748535\n",
            "training loss: 6.877558708190918\n",
            "training loss: 6.929243087768555\n",
            "training loss: 6.840601921081543\n",
            "training loss: 7.0349650382995605\n",
            "training loss: 7.026730537414551\n",
            "training loss: 7.009716033935547\n",
            "training loss: 6.9429168701171875\n",
            "training loss: 6.7915167808532715\n",
            "training loss: 6.844577789306641\n",
            "training loss: 6.850083827972412\n",
            "training loss: 6.768922805786133\n",
            "training loss: 6.92148494720459\n",
            "training loss: 6.738624095916748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  82%|████████▏ | 49/60 [30:38<07:07, 38.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.056303977966309\n",
            "training loss: 6.900996208190918\n",
            "training loss: 6.605599880218506\n",
            "training loss: 7.211894989013672\n",
            "training loss: 7.352975368499756\n",
            "training loss: 7.007073879241943\n",
            "training loss: 6.984654426574707\n",
            "training loss: 6.792144775390625\n",
            "training loss: 6.9444193840026855\n",
            "training loss: 6.976957321166992\n",
            "training loss: 7.043774604797363\n",
            "training loss: 6.981644153594971\n",
            "training loss: 6.925411224365234\n",
            "training loss: 6.757585525512695\n",
            "training loss: 6.917336463928223\n",
            "training loss: 7.128235340118408\n",
            "training loss: 7.057926177978516\n",
            "training loss: 7.008066177368164\n",
            "training loss: 7.025883674621582\n",
            "training loss: 6.878643035888672\n",
            "training loss: 6.882902145385742\n",
            "training loss: 6.772157669067383\n",
            "training loss: 6.870471000671387\n",
            "training loss: 6.859772205352783\n",
            "training loss: 7.0483856201171875\n",
            "training loss: 7.214813709259033\n",
            "training loss: 6.7816901206970215\n",
            "training loss: 7.084266662597656\n",
            "training loss: 6.957097053527832\n",
            "training loss: 6.893545627593994\n",
            "training loss: 6.980690956115723\n",
            "training loss: 6.954584121704102\n",
            "training loss: 6.756412029266357\n",
            "training loss: 6.925772190093994\n",
            "training loss: 7.038091659545898\n",
            "training loss: 6.628749847412109\n",
            "training loss: 6.867203235626221\n",
            "training loss: 6.8417205810546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  83%|████████▎ | 50/60 [31:14<06:20, 38.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.965841293334961\n",
            "training loss: 7.314207553863525\n",
            "training loss: 7.047228813171387\n",
            "training loss: 7.074230194091797\n",
            "training loss: 6.84564208984375\n",
            "training loss: 6.8047075271606445\n",
            "training loss: 7.101800441741943\n",
            "training loss: 6.964308738708496\n",
            "training loss: 6.81944465637207\n",
            "training loss: 6.9306840896606445\n",
            "training loss: 6.927302360534668\n",
            "training loss: 6.559650421142578\n",
            "training loss: 6.79295539855957\n",
            "training loss: 6.7063093185424805\n",
            "training loss: 6.5918192863464355\n",
            "training loss: 6.972661972045898\n",
            "training loss: 7.1774396896362305\n",
            "training loss: 6.8586883544921875\n",
            "training loss: 6.984889984130859\n",
            "training loss: 6.8410186767578125\n",
            "training loss: 7.030033111572266\n",
            "training loss: 6.90812873840332\n",
            "training loss: 6.621623992919922\n",
            "training loss: 6.994114875793457\n",
            "training loss: 6.911698341369629\n",
            "training loss: 6.7781877517700195\n",
            "training loss: 6.830743789672852\n",
            "training loss: 7.079012870788574\n",
            "training loss: 6.907040596008301\n",
            "training loss: 6.68096923828125\n",
            "training loss: 6.659706115722656\n",
            "training loss: 6.527641773223877\n",
            "training loss: 6.583182334899902\n",
            "training loss: 6.924245357513428\n",
            "training loss: 7.0439324378967285\n",
            "training loss: 6.68778657913208\n",
            "training loss: 6.595092296600342\n",
            "training loss: 6.812017440795898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  85%|████████▌ | 51/60 [31:52<05:43, 38.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.869901657104492\n",
            "training loss: 6.948606491088867\n",
            "training loss: 6.8111701011657715\n",
            "training loss: 7.013336181640625\n",
            "training loss: 6.968236923217773\n",
            "training loss: 6.917593002319336\n",
            "training loss: 6.819694995880127\n",
            "training loss: 6.69755220413208\n",
            "training loss: 6.733090877532959\n",
            "training loss: 6.919100284576416\n",
            "training loss: 6.865767002105713\n",
            "training loss: 6.764526844024658\n",
            "training loss: 7.115517616271973\n",
            "training loss: 6.973840713500977\n",
            "training loss: 6.7364726066589355\n",
            "training loss: 6.850704193115234\n",
            "training loss: 6.826142311096191\n",
            "training loss: 6.598993301391602\n",
            "training loss: 6.809965133666992\n",
            "training loss: 6.805530071258545\n",
            "training loss: 6.763657569885254\n",
            "training loss: 6.573249340057373\n",
            "training loss: 6.693244934082031\n",
            "training loss: 6.539745807647705\n",
            "training loss: 6.5224714279174805\n",
            "training loss: 6.721671104431152\n",
            "training loss: 6.594381332397461\n",
            "training loss: 6.507518291473389\n",
            "training loss: 6.642665386199951\n",
            "training loss: 6.861945152282715\n",
            "training loss: 6.582427024841309\n",
            "training loss: 6.760531425476074\n",
            "training loss: 6.910055160522461\n",
            "training loss: 6.711472034454346\n",
            "training loss: 6.782503128051758\n",
            "training loss: 6.795314788818359\n",
            "training loss: 6.938201904296875\n",
            "training loss: 6.790868282318115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  87%|████████▋ | 52/60 [32:31<05:06, 38.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.953996658325195\n",
            "training loss: 7.012086391448975\n",
            "training loss: 7.008606433868408\n",
            "training loss: 6.88703727722168\n",
            "training loss: 7.216002464294434\n",
            "training loss: 6.949213981628418\n",
            "training loss: 6.914943695068359\n",
            "training loss: 7.00615119934082\n",
            "training loss: 7.073986053466797\n",
            "training loss: 7.212126731872559\n",
            "training loss: 6.985317230224609\n",
            "training loss: 6.963162422180176\n",
            "training loss: 6.872418403625488\n",
            "training loss: 7.067849159240723\n",
            "training loss: 6.847853183746338\n",
            "training loss: 6.55780029296875\n",
            "training loss: 6.722249984741211\n",
            "training loss: 6.87257194519043\n",
            "training loss: 6.864795207977295\n",
            "training loss: 6.926554203033447\n",
            "training loss: 6.964598655700684\n",
            "training loss: 7.074069023132324\n",
            "training loss: 6.694724082946777\n",
            "training loss: 6.818660736083984\n",
            "training loss: 6.895662307739258\n",
            "training loss: 6.820412635803223\n",
            "training loss: 6.741372108459473\n",
            "training loss: 6.745535373687744\n",
            "training loss: 6.877806663513184\n",
            "training loss: 6.9274373054504395\n",
            "training loss: 7.041341781616211\n",
            "training loss: 7.1653594970703125\n",
            "training loss: 7.045970439910889\n",
            "training loss: 6.816397190093994\n",
            "training loss: 6.722115516662598\n",
            "training loss: 6.780324935913086\n",
            "training loss: 6.77485990524292\n",
            "training loss: 7.0346879959106445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  88%|████████▊ | 53/60 [33:10<04:29, 38.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.813329696655273\n",
            "training loss: 6.989199638366699\n",
            "training loss: 6.8991923332214355\n",
            "training loss: 7.177479267120361\n",
            "training loss: 6.909847259521484\n",
            "training loss: 7.008636474609375\n",
            "training loss: 6.760478496551514\n",
            "training loss: 6.778226852416992\n",
            "training loss: 6.821493625640869\n",
            "training loss: 6.855966567993164\n",
            "training loss: 6.891171455383301\n",
            "training loss: 6.584918975830078\n",
            "training loss: 6.797333717346191\n",
            "training loss: 7.091785430908203\n",
            "training loss: 6.93618631362915\n",
            "training loss: 6.886684417724609\n",
            "training loss: 6.915777683258057\n",
            "training loss: 6.877586364746094\n",
            "training loss: 6.974478721618652\n",
            "training loss: 6.93118953704834\n",
            "training loss: 6.868658065795898\n",
            "training loss: 6.7227983474731445\n",
            "training loss: 6.932244300842285\n",
            "training loss: 7.007365703582764\n",
            "training loss: 6.921263694763184\n",
            "training loss: 6.7524285316467285\n",
            "training loss: 6.815899848937988\n",
            "training loss: 6.925713062286377\n",
            "training loss: 6.894951820373535\n",
            "training loss: 7.063811302185059\n",
            "training loss: 6.905718803405762\n",
            "training loss: 6.952706813812256\n",
            "training loss: 6.957607746124268\n",
            "training loss: 6.939084053039551\n",
            "training loss: 6.964613914489746\n",
            "training loss: 6.717655181884766\n",
            "training loss: 6.818019390106201\n",
            "training loss: 6.7734904289245605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  90%|█████████ | 54/60 [33:49<03:51, 38.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.897148132324219\n",
            "training loss: 7.038651466369629\n",
            "training loss: 6.918470859527588\n",
            "training loss: 6.974248886108398\n",
            "training loss: 6.93233585357666\n",
            "training loss: 7.098874092102051\n",
            "training loss: 7.055475234985352\n",
            "training loss: 7.154150485992432\n",
            "training loss: 6.742842674255371\n",
            "training loss: 7.0601115226745605\n",
            "training loss: 7.063806533813477\n",
            "training loss: 7.049564361572266\n",
            "training loss: 6.812041282653809\n",
            "training loss: 6.994416236877441\n",
            "training loss: 6.94714879989624\n",
            "training loss: 6.8220977783203125\n",
            "training loss: 6.684953689575195\n",
            "training loss: 6.9729719161987305\n",
            "training loss: 6.861305236816406\n",
            "training loss: 6.705965995788574\n",
            "training loss: 6.690526962280273\n",
            "training loss: 6.650655746459961\n",
            "training loss: 6.696756839752197\n",
            "training loss: 6.855083465576172\n",
            "training loss: 6.79520320892334\n",
            "training loss: 6.752773284912109\n",
            "training loss: 6.727818489074707\n",
            "training loss: 6.9787750244140625\n",
            "training loss: 6.869909286499023\n",
            "training loss: 6.784596920013428\n",
            "training loss: 6.629397392272949\n",
            "training loss: 7.226370811462402\n",
            "training loss: 7.191652297973633\n",
            "training loss: 7.175899505615234\n",
            "training loss: 6.770709037780762\n",
            "training loss: 6.921165466308594\n",
            "training loss: 6.6273345947265625\n",
            "training loss: 6.717772483825684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  92%|█████████▏| 55/60 [34:29<03:14, 38.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.1130266189575195\n",
            "training loss: 6.756756782531738\n",
            "training loss: 6.799654006958008\n",
            "training loss: 6.665892124176025\n",
            "training loss: 6.8715386390686035\n",
            "training loss: 6.722527503967285\n",
            "training loss: 6.979198932647705\n",
            "training loss: 6.961094856262207\n",
            "training loss: 6.812318325042725\n",
            "training loss: 6.693090438842773\n",
            "training loss: 6.843992710113525\n",
            "training loss: 6.927193641662598\n",
            "training loss: 6.802081108093262\n",
            "training loss: 6.824432373046875\n",
            "training loss: 6.589081764221191\n",
            "training loss: 6.7858991622924805\n",
            "training loss: 6.911706447601318\n",
            "training loss: 6.760047912597656\n",
            "training loss: 6.952683925628662\n",
            "training loss: 6.649003982543945\n",
            "training loss: 6.835008144378662\n",
            "training loss: 6.838053226470947\n",
            "training loss: 6.6715593338012695\n",
            "training loss: 6.752882957458496\n",
            "training loss: 6.74543571472168\n",
            "training loss: 6.836582183837891\n",
            "training loss: 6.646758079528809\n",
            "training loss: 6.2886128425598145\n",
            "training loss: 6.779518127441406\n",
            "training loss: 6.579538345336914\n",
            "training loss: 6.737637996673584\n",
            "training loss: 6.836673259735107\n",
            "training loss: 6.828899383544922\n",
            "training loss: 6.772097110748291\n",
            "training loss: 6.75687313079834\n",
            "training loss: 6.832877159118652\n",
            "training loss: 6.785811901092529\n",
            "training loss: 6.925477027893066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  93%|█████████▎| 56/60 [35:07<02:34, 38.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.982007026672363\n",
            "training loss: 6.989307403564453\n",
            "training loss: 7.061033248901367\n",
            "training loss: 6.904152870178223\n",
            "training loss: 7.094211101531982\n",
            "training loss: 6.793704986572266\n",
            "training loss: 6.628725528717041\n",
            "training loss: 6.958346366882324\n",
            "training loss: 6.742885589599609\n",
            "training loss: 6.949827194213867\n",
            "training loss: 6.841962814331055\n",
            "training loss: 6.99331521987915\n",
            "training loss: 6.70922327041626\n",
            "training loss: 6.868722438812256\n",
            "training loss: 6.749872207641602\n",
            "training loss: 7.038689613342285\n",
            "training loss: 6.919381141662598\n",
            "training loss: 6.573148250579834\n",
            "training loss: 6.7870683670043945\n",
            "training loss: 6.7539825439453125\n",
            "training loss: 6.939024925231934\n",
            "training loss: 6.909376621246338\n",
            "training loss: 6.90983247756958\n",
            "training loss: 6.790401458740234\n",
            "training loss: 6.876497745513916\n",
            "training loss: 7.066905498504639\n",
            "training loss: 6.839259624481201\n",
            "training loss: 6.891239166259766\n",
            "training loss: 6.917788982391357\n",
            "training loss: 6.881349563598633\n",
            "training loss: 6.973580837249756\n",
            "training loss: 6.82973575592041\n",
            "training loss: 6.887167930603027\n",
            "training loss: 6.66754674911499\n",
            "training loss: 6.941746711730957\n",
            "training loss: 6.789446830749512\n",
            "training loss: 6.594698429107666\n",
            "training loss: 6.653088569641113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  95%|█████████▌| 57/60 [35:47<01:57, 39.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.938792705535889\n",
            "training loss: 6.935528755187988\n",
            "training loss: 7.084877967834473\n",
            "training loss: 7.000846862792969\n",
            "training loss: 6.950818061828613\n",
            "training loss: 6.812319755554199\n",
            "training loss: 6.941320419311523\n",
            "training loss: 6.764725685119629\n",
            "training loss: 6.944536209106445\n",
            "training loss: 6.782845973968506\n",
            "training loss: 6.935851097106934\n",
            "training loss: 6.660680770874023\n",
            "training loss: 6.758510589599609\n",
            "training loss: 6.587169170379639\n",
            "training loss: 6.639408111572266\n",
            "training loss: 6.803352355957031\n",
            "training loss: 6.869500160217285\n",
            "training loss: 6.60014009475708\n",
            "training loss: 6.703532695770264\n",
            "training loss: 6.747016906738281\n",
            "training loss: 6.718513488769531\n",
            "training loss: 7.130006313323975\n",
            "training loss: 6.814786434173584\n",
            "training loss: 6.9183502197265625\n",
            "training loss: 6.934237480163574\n",
            "training loss: 6.910035133361816\n",
            "training loss: 6.946595191955566\n",
            "training loss: 6.829358100891113\n",
            "training loss: 6.819521427154541\n",
            "training loss: 6.931408882141113\n",
            "training loss: 7.146778106689453\n",
            "training loss: 7.0119709968566895\n",
            "training loss: 6.79606819152832\n",
            "training loss: 6.787843227386475\n",
            "training loss: 6.881924152374268\n",
            "training loss: 6.773313522338867\n",
            "training loss: 6.734065055847168\n",
            "training loss: 6.704099178314209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  97%|█████████▋| 58/60 [36:26<01:18, 39.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 7.054348468780518\n",
            "training loss: 7.142459392547607\n",
            "training loss: 7.1833648681640625\n",
            "training loss: 7.127546787261963\n",
            "training loss: 6.903176784515381\n",
            "training loss: 7.133060455322266\n",
            "training loss: 6.965816497802734\n",
            "training loss: 6.866671562194824\n",
            "training loss: 6.8761749267578125\n",
            "training loss: 7.177804946899414\n",
            "training loss: 6.874680519104004\n",
            "training loss: 7.1550116539001465\n",
            "training loss: 6.97754430770874\n",
            "training loss: 6.87415885925293\n",
            "training loss: 6.958686828613281\n",
            "training loss: 6.97433614730835\n",
            "training loss: 6.841098785400391\n",
            "training loss: 6.917622089385986\n",
            "training loss: 6.70708703994751\n",
            "training loss: 6.721254825592041\n",
            "training loss: 6.628303527832031\n",
            "training loss: 6.982842445373535\n",
            "training loss: 6.666269779205322\n",
            "training loss: 6.659591197967529\n",
            "training loss: 6.800299644470215\n",
            "training loss: 6.885718822479248\n",
            "training loss: 6.821242809295654\n",
            "training loss: 6.63654899597168\n",
            "training loss: 6.587399005889893\n",
            "training loss: 6.659972190856934\n",
            "training loss: 6.705046653747559\n",
            "training loss: 6.774897575378418\n",
            "training loss: 6.753026485443115\n",
            "training loss: 6.655759334564209\n",
            "training loss: 6.955665588378906\n",
            "training loss: 6.885776996612549\n",
            "training loss: 6.384981632232666\n",
            "training loss: 6.72724723815918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  98%|█████████▊| 59/60 [37:07<00:39, 39.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.870663166046143\n",
            "training loss: 6.848596096038818\n",
            "training loss: 6.596922874450684\n",
            "training loss: 6.808844566345215\n",
            "training loss: 6.801584243774414\n",
            "training loss: 7.0716938972473145\n",
            "training loss: 6.928032875061035\n",
            "training loss: 6.959835052490234\n",
            "training loss: 6.795718669891357\n",
            "training loss: 6.925361156463623\n",
            "training loss: 6.756059646606445\n",
            "training loss: 6.693377494812012\n",
            "training loss: 6.721112251281738\n",
            "training loss: 7.069736957550049\n",
            "training loss: 6.804108619689941\n",
            "training loss: 6.696483612060547\n",
            "training loss: 6.8326191902160645\n",
            "training loss: 6.880479335784912\n",
            "training loss: 6.969566345214844\n",
            "training loss: 6.824099540710449\n",
            "training loss: 7.047390937805176\n",
            "training loss: 7.01374626159668\n",
            "training loss: 7.0468292236328125\n",
            "training loss: 7.003365516662598\n",
            "training loss: 6.925712585449219\n",
            "training loss: 6.7048492431640625\n",
            "training loss: 6.614938259124756\n",
            "training loss: 6.840873718261719\n",
            "training loss: 6.594534873962402\n",
            "training loss: 6.827596664428711\n",
            "training loss: 6.8206586837768555\n",
            "training loss: 6.820794582366943\n",
            "training loss: 6.932385444641113\n",
            "training loss: 6.926052093505859\n",
            "training loss: 6.565062522888184\n",
            "training loss: 6.643258571624756\n",
            "training loss: 6.523508071899414\n",
            "training loss: 6.614524841308594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 60/60 [37:46<00:00, 37.78s/it]\n",
            "evaluation: 100%|██████████| 7/7 [09:00<00:00, 77.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity: 579.7503051757812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "training:   0%|          | 0/60 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.957095146179199\n",
            "training loss: 6.974388122558594\n",
            "training loss: 6.81735897064209\n",
            "training loss: 6.81107234954834\n",
            "training loss: 6.687056064605713\n",
            "training loss: 6.9996185302734375\n",
            "training loss: 6.944690704345703\n",
            "training loss: 7.12861442565918\n",
            "training loss: 6.683782577514648\n",
            "training loss: 6.711423873901367\n",
            "training loss: 6.751982688903809\n",
            "training loss: 6.879479885101318\n",
            "training loss: 6.833583831787109\n",
            "training loss: 6.823814868927002\n",
            "training loss: 7.041652679443359\n",
            "training loss: 6.866843223571777\n",
            "training loss: 6.87339973449707\n",
            "training loss: 6.834296703338623\n",
            "training loss: 6.739772796630859\n",
            "training loss: 6.917514801025391\n",
            "training loss: 7.028114318847656\n",
            "training loss: 6.868287563323975\n",
            "training loss: 6.799604892730713\n",
            "training loss: 6.913195610046387\n",
            "training loss: 6.7120513916015625\n",
            "training loss: 6.7668585777282715\n",
            "training loss: 6.833343982696533\n",
            "training loss: 6.673567771911621\n",
            "training loss: 6.599002838134766\n",
            "training loss: 6.659074783325195\n",
            "training loss: 6.52410888671875\n",
            "training loss: 6.679678916931152\n",
            "training loss: 6.562241554260254\n",
            "training loss: 6.5722432136535645\n",
            "training loss: 6.383063316345215\n",
            "training loss: 6.6238508224487305\n",
            "training loss: 6.477262020111084\n",
            "training loss: 6.28408670425415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   2%|▏         | 1/60 [00:38<37:23, 38.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.633444786071777\n",
            "training loss: 6.854423522949219\n",
            "training loss: 7.1414384841918945\n",
            "training loss: 6.84889030456543\n",
            "training loss: 6.8224358558654785\n",
            "training loss: 6.721945762634277\n",
            "training loss: 6.875545501708984\n",
            "training loss: 6.596056938171387\n",
            "training loss: 6.882552146911621\n",
            "training loss: 6.728549957275391\n",
            "training loss: 6.787110328674316\n",
            "training loss: 6.472787857055664\n",
            "training loss: 6.745189189910889\n",
            "training loss: 6.896800994873047\n",
            "training loss: 6.706692218780518\n",
            "training loss: 6.53655481338501\n",
            "training loss: 6.55584716796875\n",
            "training loss: 6.523585319519043\n",
            "training loss: 6.579895973205566\n",
            "training loss: 6.738574981689453\n",
            "training loss: 6.623417377471924\n",
            "training loss: 6.636842727661133\n",
            "training loss: 6.615724563598633\n",
            "training loss: 6.941981792449951\n",
            "training loss: 6.738948822021484\n",
            "training loss: 6.680636405944824\n",
            "training loss: 6.617753982543945\n",
            "training loss: 6.800180435180664\n",
            "training loss: 6.628800392150879\n",
            "training loss: 6.665338516235352\n",
            "training loss: 6.304229736328125\n",
            "training loss: 6.563323974609375\n",
            "training loss: 6.5128493309021\n",
            "training loss: 6.654967308044434\n",
            "training loss: 6.793500900268555\n",
            "training loss: 6.5387372970581055\n",
            "training loss: 6.6745452880859375\n",
            "training loss: 6.99421501159668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   3%|▎         | 2/60 [01:19<38:54, 40.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.678285598754883\n",
            "training loss: 6.7176408767700195\n",
            "training loss: 6.753331661224365\n",
            "training loss: 6.816427707672119\n",
            "training loss: 6.757762908935547\n",
            "training loss: 6.616111755371094\n",
            "training loss: 6.777112007141113\n",
            "training loss: 6.955469131469727\n",
            "training loss: 6.727673530578613\n",
            "training loss: 6.543127536773682\n",
            "training loss: 6.52900505065918\n",
            "training loss: 6.754167556762695\n",
            "training loss: 6.827188968658447\n",
            "training loss: 6.734192848205566\n",
            "training loss: 6.5459442138671875\n",
            "training loss: 6.524453163146973\n",
            "training loss: 6.717036247253418\n",
            "training loss: 6.697606563568115\n",
            "training loss: 6.668074131011963\n",
            "training loss: 6.530206680297852\n",
            "training loss: 6.671092510223389\n",
            "training loss: 6.502136707305908\n",
            "training loss: 6.7082929611206055\n",
            "training loss: 6.506166458129883\n",
            "training loss: 6.567440032958984\n",
            "training loss: 6.462892055511475\n",
            "training loss: 6.6212687492370605\n",
            "training loss: 6.403895378112793\n",
            "training loss: 6.282199382781982\n",
            "training loss: 6.4604411125183105\n",
            "training loss: 6.616903305053711\n",
            "training loss: 6.714500427246094\n",
            "training loss: 6.86861515045166\n",
            "training loss: 6.577211856842041\n",
            "training loss: 6.78843355178833\n",
            "training loss: 6.679186820983887\n",
            "training loss: 6.825073719024658\n",
            "training loss: 6.699402332305908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   5%|▌         | 3/60 [01:56<36:28, 38.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.481396675109863\n",
            "training loss: 6.687047958374023\n",
            "training loss: 6.922365188598633\n",
            "training loss: 6.831699371337891\n",
            "training loss: 6.896409511566162\n",
            "training loss: 6.736261367797852\n",
            "training loss: 6.87237024307251\n",
            "training loss: 6.747863292694092\n",
            "training loss: 6.556361198425293\n",
            "training loss: 6.606446743011475\n",
            "training loss: 6.561744689941406\n",
            "training loss: 6.612799167633057\n",
            "training loss: 6.872845649719238\n",
            "training loss: 6.5153398513793945\n",
            "training loss: 6.449522495269775\n",
            "training loss: 6.551185607910156\n",
            "training loss: 6.553907871246338\n",
            "training loss: 6.75395393371582\n",
            "training loss: 6.543267726898193\n",
            "training loss: 6.622194290161133\n",
            "training loss: 6.569707870483398\n",
            "training loss: 6.676532745361328\n",
            "training loss: 6.7738542556762695\n",
            "training loss: 6.352170944213867\n",
            "training loss: 6.729585647583008\n",
            "training loss: 6.716794967651367\n",
            "training loss: 6.828850746154785\n",
            "training loss: 6.584221839904785\n",
            "training loss: 6.582246780395508\n",
            "training loss: 6.798163414001465\n",
            "training loss: 6.572643280029297\n",
            "training loss: 6.6577277183532715\n",
            "training loss: 6.670843601226807\n",
            "training loss: 6.714786529541016\n",
            "training loss: 6.7188639640808105\n",
            "training loss: 6.611112594604492\n",
            "training loss: 6.533326625823975\n",
            "training loss: 6.574378967285156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   7%|▋         | 4/60 [02:31<34:56, 37.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.970897197723389\n",
            "training loss: 6.4881062507629395\n",
            "training loss: 6.692089080810547\n",
            "training loss: 6.7793049812316895\n",
            "training loss: 6.484671592712402\n",
            "training loss: 6.73659610748291\n",
            "training loss: 6.783583641052246\n",
            "training loss: 6.5859832763671875\n",
            "training loss: 6.38684606552124\n",
            "training loss: 6.964719772338867\n",
            "training loss: 6.868992805480957\n",
            "training loss: 6.998292922973633\n",
            "training loss: 6.884632110595703\n",
            "training loss: 6.799510478973389\n",
            "training loss: 6.754396438598633\n",
            "training loss: 6.841404914855957\n",
            "training loss: 6.876728534698486\n",
            "training loss: 6.856775283813477\n",
            "training loss: 6.986711502075195\n",
            "training loss: 6.820898056030273\n",
            "training loss: 7.080475807189941\n",
            "training loss: 6.820551872253418\n",
            "training loss: 6.910965919494629\n",
            "training loss: 6.6005096435546875\n",
            "training loss: 6.700996398925781\n",
            "training loss: 6.377022743225098\n",
            "training loss: 6.668941497802734\n",
            "training loss: 6.74826192855835\n",
            "training loss: 6.8620381355285645\n",
            "training loss: 6.6965556144714355\n",
            "training loss: 6.8261260986328125\n",
            "training loss: 6.89406681060791\n",
            "training loss: 6.639734268188477\n",
            "training loss: 6.474521160125732\n",
            "training loss: 6.632565498352051\n",
            "training loss: 6.490361213684082\n",
            "training loss: 6.238170146942139\n",
            "training loss: 6.677258014678955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:   8%|▊         | 5/60 [03:11<34:56, 38.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.569098472595215\n",
            "training loss: 6.5940446853637695\n",
            "training loss: 7.024827480316162\n",
            "training loss: 6.891556739807129\n",
            "training loss: 6.780222415924072\n",
            "training loss: 6.4051971435546875\n",
            "training loss: 6.573558807373047\n",
            "training loss: 6.537386894226074\n",
            "training loss: 6.868372440338135\n",
            "training loss: 6.721973896026611\n",
            "training loss: 6.674764156341553\n",
            "training loss: 6.695356845855713\n",
            "training loss: 6.473511219024658\n",
            "training loss: 6.627732276916504\n",
            "training loss: 6.708927631378174\n",
            "training loss: 6.626855373382568\n",
            "training loss: 6.490669250488281\n",
            "training loss: 6.362893581390381\n",
            "training loss: 6.264203071594238\n",
            "training loss: 6.380334854125977\n",
            "training loss: 6.615744590759277\n",
            "training loss: 6.730640888214111\n",
            "training loss: 6.609063148498535\n",
            "training loss: 6.655846118927002\n",
            "training loss: 6.745724201202393\n",
            "training loss: 6.551280975341797\n",
            "training loss: 6.599545955657959\n",
            "training loss: 6.557398796081543\n",
            "training loss: 6.585271835327148\n",
            "training loss: 6.724896430969238\n",
            "training loss: 6.807224273681641\n",
            "training loss: 6.779495716094971\n",
            "training loss: 6.490434646606445\n",
            "training loss: 6.606539726257324\n",
            "training loss: 6.647749900817871\n",
            "training loss: 6.51210880279541\n",
            "training loss: 6.5810933113098145\n",
            "training loss: 6.459157943725586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  10%|█         | 6/60 [03:50<34:33, 38.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.974701881408691\n",
            "training loss: 6.964097023010254\n",
            "training loss: 6.903111457824707\n",
            "training loss: 6.892504692077637\n",
            "training loss: 6.738393783569336\n",
            "training loss: 6.973824977874756\n",
            "training loss: 6.91106653213501\n",
            "training loss: 6.852544784545898\n",
            "training loss: 6.771493434906006\n",
            "training loss: 6.655978202819824\n",
            "training loss: 6.753813743591309\n",
            "training loss: 6.4971818923950195\n",
            "training loss: 6.673310279846191\n",
            "training loss: 6.86073112487793\n",
            "training loss: 6.934242248535156\n",
            "training loss: 6.736698150634766\n",
            "training loss: 6.496304512023926\n",
            "training loss: 6.757180213928223\n",
            "training loss: 6.9113874435424805\n",
            "training loss: 6.828494071960449\n",
            "training loss: 6.548844337463379\n",
            "training loss: 6.597064018249512\n",
            "training loss: 6.883923530578613\n",
            "training loss: 6.664388656616211\n",
            "training loss: 6.6179704666137695\n",
            "training loss: 6.675411224365234\n",
            "training loss: 6.730632781982422\n",
            "training loss: 6.478757381439209\n",
            "training loss: 6.887681484222412\n",
            "training loss: 6.659327983856201\n",
            "training loss: 6.705287933349609\n",
            "training loss: 6.779515266418457\n",
            "training loss: 6.935310363769531\n",
            "training loss: 6.678038120269775\n",
            "training loss: 6.599715232849121\n",
            "training loss: 6.799462795257568\n",
            "training loss: 6.489223957061768\n",
            "training loss: 6.443781852722168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  12%|█▏        | 7/60 [04:30<34:21, 38.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.778201580047607\n",
            "training loss: 6.682707786560059\n",
            "training loss: 6.860804557800293\n",
            "training loss: 6.805787086486816\n",
            "training loss: 6.790578842163086\n",
            "training loss: 6.9449615478515625\n",
            "training loss: 6.703210830688477\n",
            "training loss: 6.58353328704834\n",
            "training loss: 6.642594337463379\n",
            "training loss: 6.526101112365723\n",
            "training loss: 6.3939361572265625\n",
            "training loss: 6.4788031578063965\n",
            "training loss: 6.7144293785095215\n",
            "training loss: 6.440793991088867\n",
            "training loss: 6.468563079833984\n",
            "training loss: 6.429896354675293\n",
            "training loss: 6.421083450317383\n",
            "training loss: 6.527939319610596\n",
            "training loss: 6.193029880523682\n",
            "training loss: 6.501580238342285\n",
            "training loss: 6.518566131591797\n",
            "training loss: 6.601311206817627\n",
            "training loss: 6.5506463050842285\n",
            "training loss: 6.525503158569336\n",
            "training loss: 6.473976135253906\n",
            "training loss: 6.725988388061523\n",
            "training loss: 6.418148994445801\n",
            "training loss: 6.795740127563477\n",
            "training loss: 6.602352142333984\n",
            "training loss: 6.661957740783691\n",
            "training loss: 6.447260856628418\n",
            "training loss: 6.749671459197998\n",
            "training loss: 6.848754405975342\n",
            "training loss: 6.562956809997559\n",
            "training loss: 6.596053600311279\n",
            "training loss: 6.546375751495361\n",
            "training loss: 6.439762115478516\n",
            "training loss: 6.6343994140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  13%|█▎        | 8/60 [05:07<33:23, 38.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.9241461753845215\n",
            "training loss: 7.08669900894165\n",
            "training loss: 6.82155704498291\n",
            "training loss: 6.841423988342285\n",
            "training loss: 6.717539310455322\n",
            "training loss: 6.78546142578125\n",
            "training loss: 6.559330463409424\n",
            "training loss: 6.752979755401611\n",
            "training loss: 6.886559963226318\n",
            "training loss: 6.851531982421875\n",
            "training loss: 6.869381904602051\n",
            "training loss: 6.740644454956055\n",
            "training loss: 6.643796920776367\n",
            "training loss: 6.71805477142334\n",
            "training loss: 6.7651824951171875\n",
            "training loss: 6.887016773223877\n",
            "training loss: 6.622503280639648\n",
            "training loss: 6.461225509643555\n",
            "training loss: 6.67803955078125\n",
            "training loss: 6.67514705657959\n",
            "training loss: 6.786848545074463\n",
            "training loss: 6.536880016326904\n",
            "training loss: 6.671873092651367\n",
            "training loss: 6.680593490600586\n",
            "training loss: 6.767568111419678\n",
            "training loss: 6.725917816162109\n",
            "training loss: 6.617982864379883\n",
            "training loss: 6.884876251220703\n",
            "training loss: 6.894595146179199\n",
            "training loss: 6.665920257568359\n",
            "training loss: 6.689505100250244\n",
            "training loss: 6.710110664367676\n",
            "training loss: 6.702866554260254\n",
            "training loss: 6.627662181854248\n",
            "training loss: 6.673493385314941\n",
            "training loss: 6.812883377075195\n",
            "training loss: 6.673142910003662\n",
            "training loss: 6.875486373901367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  15%|█▌        | 9/60 [05:43<32:00, 37.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.8504638671875\n",
            "training loss: 7.043081283569336\n",
            "training loss: 6.665017127990723\n",
            "training loss: 6.45582914352417\n",
            "training loss: 6.5388898849487305\n",
            "training loss: 6.827081680297852\n",
            "training loss: 6.772106647491455\n",
            "training loss: 6.725342273712158\n",
            "training loss: 6.675760269165039\n",
            "training loss: 6.404898643493652\n",
            "training loss: 6.641576766967773\n",
            "training loss: 6.626163482666016\n",
            "training loss: 6.327413558959961\n",
            "training loss: 6.198956489562988\n",
            "training loss: 6.025440216064453\n",
            "training loss: 6.240384101867676\n",
            "training loss: 6.470192909240723\n",
            "training loss: 6.440708160400391\n",
            "training loss: 6.709847450256348\n",
            "training loss: 6.879680156707764\n",
            "training loss: 6.752749443054199\n",
            "training loss: 6.781670570373535\n",
            "training loss: 6.961189270019531\n",
            "training loss: 6.746035575866699\n",
            "training loss: 6.823333740234375\n",
            "training loss: 6.681046009063721\n",
            "training loss: 6.927159309387207\n",
            "training loss: 6.954821586608887\n",
            "training loss: 6.921176433563232\n",
            "training loss: 6.644864082336426\n",
            "training loss: 6.514411449432373\n",
            "training loss: 6.794177055358887\n",
            "training loss: 6.588316917419434\n",
            "training loss: 6.7550578117370605\n",
            "training loss: 6.630975246429443\n",
            "training loss: 6.6396164894104\n",
            "training loss: 6.767751216888428\n",
            "training loss: 6.617483139038086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  17%|█▋        | 10/60 [06:18<30:44, 36.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.838354110717773\n",
            "training loss: 6.706753253936768\n",
            "training loss: 6.84565544128418\n",
            "training loss: 6.798364162445068\n",
            "training loss: 6.898402214050293\n",
            "training loss: 6.679961681365967\n",
            "training loss: 6.7138895988464355\n",
            "training loss: 6.899338722229004\n",
            "training loss: 6.530026912689209\n",
            "training loss: 6.930781364440918\n",
            "training loss: 6.6954169273376465\n",
            "training loss: 6.97786808013916\n",
            "training loss: 6.785508155822754\n",
            "training loss: 6.666471004486084\n",
            "training loss: 6.838608264923096\n",
            "training loss: 6.682365894317627\n",
            "training loss: 6.651714324951172\n",
            "training loss: 6.519839763641357\n",
            "training loss: 6.575697898864746\n",
            "training loss: 6.571249961853027\n",
            "training loss: 6.708641529083252\n",
            "training loss: 6.797478675842285\n",
            "training loss: 6.772987365722656\n",
            "training loss: 6.7797393798828125\n",
            "training loss: 6.789322376251221\n",
            "training loss: 6.788727283477783\n",
            "training loss: 6.623803615570068\n",
            "training loss: 6.6737165451049805\n",
            "training loss: 6.7123823165893555\n",
            "training loss: 6.760324954986572\n",
            "training loss: 6.686395645141602\n",
            "training loss: 6.494546413421631\n",
            "training loss: 6.728697776794434\n",
            "training loss: 6.769807815551758\n",
            "training loss: 6.533559322357178\n",
            "training loss: 6.466714382171631\n",
            "training loss: 6.754739761352539\n",
            "training loss: 6.622403144836426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtraining:  18%|█▊        | 11/60 [06:54<29:42, 36.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 6.781742095947266\n",
            "training loss: 6.974839687347412\n",
            "training loss: 6.893237113952637\n",
            "training loss: 6.53061580657959\n",
            "training loss: 6.769012451171875\n",
            "training loss: 6.7729926109313965\n",
            "training loss: 6.688362121582031\n",
            "training loss: 6.7340803146362305\n",
            "training loss: 6.622211933135986\n",
            "training loss: 6.678520202636719\n",
            "training loss: 6.814720153808594\n",
            "training loss: 6.609689712524414\n",
            "training loss: 6.620741367340088\n",
            "training loss: 6.54838228225708\n",
            "training loss: 6.669677734375\n",
            "training loss: 6.624885559082031\n",
            "training loss: 6.51583194732666\n",
            "training loss: 6.678399085998535\n",
            "training loss: 6.400171756744385\n",
            "training loss: 6.636236190795898\n",
            "training loss: 6.748119354248047\n",
            "training loss: 6.785141944885254\n",
            "training loss: 6.76462459564209\n",
            "training loss: 6.828683853149414\n",
            "training loss: 6.862295627593994\n",
            "training loss: 6.708677291870117\n",
            "training loss: 6.687836170196533\n",
            "training loss: 6.694792747497559\n",
            "training loss: 6.821475982666016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#Luigi\n",
        "perplexity_list = []\n",
        "with open(f'drive/MyDrive/Colab Notebooks/perplexity_memorizing_tr.pkl', 'rb') as pklfile:\n",
        "  perplexity_list = pkl.load(pklfile)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "16-Mkrn0WGN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(perplexity_list, label = \"Memorizing Transformer Perplexity Plot\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n1DiMQgKWJnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer decoder training"
      ],
      "metadata": {
        "id": "ayPdgmodFg4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module): #decoder-only architecture of the transformer\n",
        "  def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        d,\n",
        "        heads = 8,\n",
        "        depth = 4,\n",
        "        hidden_size = 1000,\n",
        "        dropout = 0.3,\n",
        "        batch_size = 16,\n",
        "        use_bert = True\n",
        "    ):\n",
        "      # asserts\n",
        "      self.d = d if not use_bert else 768\n",
        "      assert self.d % heads == 0\n",
        " \n",
        "      super(TransformerDecoder, self).__init__()\n",
        "      self.token_emb = nn.Embedding(num_tokens, self.d)\n",
        "      self.positional_enc = PositionalEncoding(self.d, max_len = 5000)\n",
        "      self.dim_head = self.d // heads\n",
        "      self.heads = heads\n",
        "      self.depth = depth\n",
        "      self.hidden_size = hidden_size\n",
        "      self.dropout = dropout\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "      self.layers = nn.ModuleList([])\n",
        "      for idx in range(depth):\n",
        "          self.layers.append(\n",
        "              TransformerBlock(d, heads, batch_size, hidden_size, dropout)\n",
        "          )\n",
        "\n",
        "      self.to_out = nn.Linear(d, num_tokens)\n",
        "    \n",
        "  def create_mask(self, x):\n",
        "    batch_size, seq_len = x.shape\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len))).expand(\n",
        "        batch_size, 1, seq_len, seq_len\n",
        "    )\n",
        "    return mask \n",
        "          \n",
        "  def forward(self, x):\n",
        "    mask = self.create_mask(x)\n",
        "\n",
        "    x = self.token_emb(x)\n",
        "    x = self.positional_enc(x)\n",
        "\n",
        "    for idx in range(self.depth):\n",
        "        x= self.layers[idx](x, mask)\n",
        "\n",
        "    return self.to_out(x).transpose(1, 2)"
      ],
      "metadata": {
        "id": "lBz91btCHjXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "SEQ_LEN = 256\n",
        "SEGMENTS = 5\n",
        "HEADS = 8\n",
        "DIM_HEAD = SEQ_LEN // HEADS\n",
        "DIM_HEAD_BERT = 768 // HEADS\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_GRAD_CLIP_NORM = 0.5\n",
        "\n",
        "EVAL_EVERY = 20\n",
        "CHECKPOINT = 5"
      ],
      "metadata": {
        "id": "sF-Xk7gtKMA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tr_decoder = TransformerDecoder(\n",
        "    num_tokens = vocabulary,\n",
        "    d = SEQ_LEN,\n",
        "    depth = 4,\n",
        "    heads = HEADS,\n",
        "    hidden_size = 5000,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    use_bert = True\n",
        ").to(device)\n",
        "\n",
        "train_loader_ = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)\n",
        "test_loader_ = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False, drop_last = True)"
      ],
      "metadata": {
        "id": "GOz3oBuiGkYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "optimizer = torch.optim.Adam(tr_decoder.parameters(), lr = LEARNING_RATE)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "epochs = 5\n",
        "# training\n",
        "\n",
        "perplexity_tr_decoder = []\n",
        "\n",
        "for e in range(epochs):\n",
        "  for i, data in enumerate(tqdm.tqdm(train_loader_, desc = 'training')):\n",
        "    tr_decoder.train()\n",
        "\n",
        "    train_loss = 0.\n",
        "\n",
        "    num_seq = 10000 // (SEQ_LEN + 1)\n",
        "    data = data.long().to(device)\n",
        "    for j in range(num_seq):\n",
        "      mini_batch = data[:, j*(SEQ_LEN + 1):(j+1)*(SEQ_LEN + 1)]\n",
        "      seq, labels = mini_batch[:, :-1], mini_batch[:, 1:]\n",
        "\n",
        "      out = tr_decoder(seq)\n",
        "\n",
        "      loss_item = loss(out, labels)\n",
        "      print(f'training loss: {loss_item}', flush = True)\n",
        "      loss_item.backward() \n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "  data = None\n",
        "    \n",
        "\n",
        "  if e % EVAL_EVERY == 0:\n",
        "    tr_decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      metric = Perplexity().to(device)\n",
        "      for i, data in enumerate(tqdm.tqdm(test_loader_, desc = 'evaluation')):\n",
        "        num_seq = 10000 // (SEQ_LEN + 1)\n",
        "        data = data.long().to(device)\n",
        "\n",
        "        for j in range(num_seq):\n",
        "          mini_batch = data[:, j*(SEQ_LEN + 1):(j+1)*(SEQ_LEN + 1)]\n",
        "          seq, labels = mini_batch[:, :-1], mini_batch[:, 1:]\n",
        "\n",
        "          out = tr_decoder(seq)\n",
        "\n",
        "          test_loss = loss(out, labels)\n",
        "          metric(out.transpose(1, 2), labels)\n",
        "          print(f'test loss: {test_loss}', flush = True)\n",
        "\n",
        "      perplexity = metric.compute()\n",
        "      perplexity_tr_decoder.append(perplexity.to(\"cpu\").item())\n",
        "      print(f'perplexity: {perplexity}', flush = True)\n",
        "\n",
        "  data = None\n",
        "  if e % CHECKPOINT == 0:\n",
        "    torch.save({\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, 'model_optimizer.pt')\n",
        "    \"\"\"\n",
        "    #Lorenzo\n",
        "    with open('/content/drive/MyDrive/Università/Magistrale/Secondo Anno/Neural Networks/project/perplexity_moreNN.npy', 'wb') as f:\n",
        "      np.save(f, np.array(perplexity_list))\n",
        "    \"\"\"\n",
        "    #Luigi\n",
        "    with open(f'drive/MyDrive/Colab Notebooks/perplexity_tr_decoder.pkl', 'wb') as pklfile:\n",
        "      pkl.dump(perplexity_tr_decoder, pklfile)"
      ],
      "metadata": {
        "id": "tqKuP2z1BeFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "perplexity_tr_decoder = []\n",
        "with open(f'drive/MyDrive/Colab Notebooks/perplexity_tr_decoder.pkl', 'rb') as pklfile:\n",
        "  perplexity_tr_decoder = pkl.load(pklfile)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QcYwi7zKXre0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "89d72de5-e537-4baf-ff4f-1cf5caf6b5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nperplexity_tr_decoder = []\\nwith open(f'drive/MyDrive/Colab Notebooks/perplexity_tr_decoder.pkl', 'rb') as pklfile:\\n  perplexity_tr_decoder = pkl.load(pklfile)\\n  pklfile.close()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(perplexity_tr_decoder, label = 'Perplexity transformer decoder')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1rb7IdThijCp",
        "outputId": "adb7e5eb-6ed9-4d18-a850-d370c7554260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "zzz"
      ],
      "metadata": {
        "id": "uTPL4pa61mUs"
      }
    }
  ]
}